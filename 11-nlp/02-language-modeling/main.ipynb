{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e7c1169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/malick/miniconda3/envs/pt/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from datasets import load_dataset\n",
    "import markov_chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bdfebf",
   "metadata": {},
   "source": [
    "# Language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9774bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\n",
    "    \"<s> I am Sam </s>\",\n",
    "    \"<s> Sam I am </s>\",\n",
    "    \"<s> I do not like green eggs and ham </s>\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6375aeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1grams: 100%|██████████| 3/3 [00:00<00:00, 20068.44it/s]\n",
      "bag of 1grams: 3it [00:00, 1660.67it/s]\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = utils.bag_of_ngrams(corpus=corpus, n=1, tokenize_function=utils.split_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a229b379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;s&gt;</th>\n",
       "      <th>I</th>\n",
       "      <th>&lt;/s&gt;</th>\n",
       "      <th>am</th>\n",
       "      <th>Sam</th>\n",
       "      <th>do</th>\n",
       "      <th>not</th>\n",
       "      <th>like</th>\n",
       "      <th>green</th>\n",
       "      <th>eggs</th>\n",
       "      <th>and</th>\n",
       "      <th>ham</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   <s>    I  </s>   am  Sam   do  not  like  green  eggs  and  ham\n",
       "0  1.0  1.0   1.0  1.0  1.0  0.0  0.0   0.0    0.0   0.0  0.0  0.0\n",
       "1  1.0  1.0   1.0  1.0  1.0  0.0  0.0   0.0    0.0   0.0  0.0  0.0\n",
       "2  1.0  1.0   1.0  0.0  0.0  1.0  1.0   1.0    1.0   1.0  1.0  1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f535ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2grams: 100%|██████████| 3/3 [00:00<00:00, 17898.88it/s]\n",
      "bag of 2grams: 3it [00:00, 1308.00it/s]\n"
     ]
    }
   ],
   "source": [
    "bigram = utils.bag_of_ngrams(corpus=corpus, n=2, tokenize_function=utils.split_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3058f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;s&gt; I</th>\n",
       "      <th>I am</th>\n",
       "      <th>am Sam</th>\n",
       "      <th>Sam &lt;/s&gt;</th>\n",
       "      <th>&lt;s&gt; Sam</th>\n",
       "      <th>Sam I</th>\n",
       "      <th>am &lt;/s&gt;</th>\n",
       "      <th>I do</th>\n",
       "      <th>do not</th>\n",
       "      <th>not like</th>\n",
       "      <th>like green</th>\n",
       "      <th>green eggs</th>\n",
       "      <th>eggs and</th>\n",
       "      <th>and ham</th>\n",
       "      <th>ham &lt;/s&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   <s> I  I am  am Sam  Sam </s>  <s> Sam  Sam I  am </s>  I do  do not  \\\n",
       "0    1.0   1.0     1.0       1.0      0.0    0.0      0.0   0.0     0.0   \n",
       "1    0.0   1.0     0.0       0.0      1.0    1.0      1.0   0.0     0.0   \n",
       "2    1.0   0.0     0.0       0.0      0.0    0.0      0.0   1.0     1.0   \n",
       "\n",
       "   not like  like green  green eggs  eggs and  and ham  ham </s>  \n",
       "0       0.0         0.0         0.0       0.0      0.0       0.0  \n",
       "1       0.0         0.0         0.0       0.0      0.0       0.0  \n",
       "2       1.0         1.0         1.0       1.0      1.0       1.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19423d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_compute_pw1_knowing_w2(w2, w1, bag_of_words, bigram):\n",
    "    p = bigram[f\"{w1} {w2}\"].sum() / bag_of_words[w1].sum()\n",
    "    print(f\"p({w2}|{w1})={p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcd4e8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(I|<s>)=0.6667\n",
      "p(</s>|Sam)=0.5000\n",
      "p(eggs|green)=1.0000\n",
      "p(Sam|am)=0.5000\n",
      "p(am|I)=0.6667\n",
      "p(do|I)=0.3333\n"
     ]
    }
   ],
   "source": [
    "ngrams_compute_pw1_knowing_w2(w2=\"I\", w1=\"<s>\", bigram=bigram, bag_of_words=bag_of_words)\n",
    "ngrams_compute_pw1_knowing_w2(w2=\"</s>\", w1=\"Sam\", bigram=bigram, bag_of_words=bag_of_words)\n",
    "ngrams_compute_pw1_knowing_w2(w2=\"eggs\", w1=\"green\", bigram=bigram, bag_of_words=bag_of_words)\n",
    "ngrams_compute_pw1_knowing_w2(w2=\"Sam\", w1=\"am\", bigram=bigram, bag_of_words=bag_of_words)\n",
    "ngrams_compute_pw1_knowing_w2(w2=\"am\", w1=\"I\", bigram=bigram, bag_of_words=bag_of_words)\n",
    "ngrams_compute_pw1_knowing_w2(w2=\"do\", w1=\"I\", bigram=bigram, bag_of_words=bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c23e374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', 'I', 'am', 'Sam', '</s>'],\n",
       " ['<s>', 'Sam', 'I', 'am', '</s>'],\n",
       " ['<s>', 'I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '</s>']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus, words_freq = utils.split_tokenize(corpus=corpus)\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feaedc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = list(words_freq.keys())\n",
    "markov_model = markov_chains.MarkovChains(states=states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a30d079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>',\n",
       " '<s>',\n",
       " 'I',\n",
       " 'Sam',\n",
       " 'am',\n",
       " 'and',\n",
       " 'do',\n",
       " 'eggs',\n",
       " 'green',\n",
       " 'ham',\n",
       " 'like',\n",
       " 'not']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markov_model.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75716388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08333334, 0.08333334, 0.08333334, 0.08333334, 0.08333334,\n",
       "        0.08333334, 0.08333334, 0.08333334, 0.08333334, 0.08333334,\n",
       "        0.08333334, 0.08333334],\n",
       "       [0.        , 0.        , 0.6666667 , 0.33333334, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.6666667 ,\n",
       "        0.        , 0.33333334, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.5       , 0.        , 0.5       , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.5       , 0.        , 0.        , 0.5       , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markov_model.fit(tokenized_corpus=tokenized_corpus)\n",
    "markov_model.transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e12fd64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99999994, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markov_model.transition_matrix.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fc9c8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probabale next states:\n",
      "Current state: [</s>]  | next state: [</s>]  | with proba: p(</s>|</s>)=0.0833\n",
      "Current state: [<s>]   | next state: [I]     | with proba: p(<s>|I)=0.6667\n",
      "Current state: [I]     | next state: [am]    | with proba: p(I|am)=0.6667\n",
      "Current state: [Sam]   | next state: [</s>]  | with proba: p(Sam|</s>)=0.5000\n",
      "Current state: [am]    | next state: [</s>]  | with proba: p(am|</s>)=0.5000\n",
      "Current state: [and]   | next state: [ham]   | with proba: p(and|ham)=1.0000\n",
      "Current state: [do]    | next state: [not]   | with proba: p(do|not)=1.0000\n",
      "Current state: [eggs]  | next state: [and]   | with proba: p(eggs|and)=1.0000\n",
      "Current state: [green] | next state: [eggs]  | with proba: p(green|eggs)=1.0000\n",
      "Current state: [ham]   | next state: [</s>]  | with proba: p(ham|</s>)=1.0000\n",
      "Current state: [like]  | next state: [green] | with proba: p(like|green)=1.0000\n",
      "Current state: [not]   | next state: [like]  | with proba: p(not|like)=1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Most probabale next states:\")\n",
    "for current_state in markov_model.states:\n",
    "    next_state, probas = markov_model.predict_next_state(current_state=current_state)\n",
    "    print(f\"Current state: [{current_state}]\".ljust(23)\n",
    "        + f\"| next state: [{next_state}]\".ljust(22)\n",
    "        + f\"| with proba: p({current_state}|{next_state})={probas.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fd9a423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate sequences:\n",
      "Seq1: <s> I am </s> </s> </s> </s> </s> </s> </s>\n",
      "Seq2: <s> I am </s> </s> </s> </s> </s> </s> </s>\n",
      "Seq3: I am </s> </s> </s> </s> </s> </s> </s> </s>\n"
     ]
    }
   ],
   "source": [
    "print(\"Generate sequences:\")\n",
    "print(\"Seq1:\", \" \".join(markov_model.generate(start=None, length=10)))\n",
    "print(\"Seq2:\", \" \".join(markov_model.generate(start=\"<s>\", length=10)))\n",
    "print(\"Seq3:\", \" \".join(markov_model.generate(start=\"I\", length=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b665b51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:\n",
      "machine translation : p(<s> I am)=0.4444 > p(<s> am I)=0.0000\n",
      "spell correction    : p(<s> I do not like)=0.2222 > p(<s> I do not lik)=0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation:\")\n",
    "\n",
    "data = [\n",
    "    (\"machine translation\", \"<s> I am\", \"<s> am I\"),\n",
    "    (\"spell correction\", \"<s> I do not like\", \"<s> I do not lik\"),\n",
    "]\n",
    "\n",
    "for title, text1, text2 in data:\n",
    "    score1 = markov_model.score_sequence(sequence=text1.split())\n",
    "    score2 = markov_model.score_sequence(sequence=text2.split())\n",
    "    print(f\"{title.ljust(20)}: p({text1})={score1:.4f} > p({text2})={score2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f12df898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', 'I', 'am', 'Sam', '</s>'],\n",
       " ['<s>', 'Sam', 'I', 'am', '</s>'],\n",
       " ['<s>', 'I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '</s>']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5843f8fc",
   "metadata": {},
   "source": [
    "Using Wolof corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5add332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['french', 'wolof', 'sources'],\n",
       "    num_rows: 17777\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = load_dataset(\"galsenai/french-wolof-translation\")[\"train\"]\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b60a684a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bataaxal bii jëwriñu lu ajju ci mbiru bitim réew bu Ekuwatër moo ko wara wóoral, te dafa wara mengoo ak yenni càkuteef yi.\\n',\n",
       " '\"Amuñu woon benn jot ngir rawal sunu bopp.\\n',\n",
       " 'Ekost sa dëkk la te bëggna nu nga toog fi.\"\\n',\n",
       " 'Xibaari Jotna : Espaañ joxe na juróom-ñett-fukki milyaar ak ñeent ci xaalisu Seefa ngir dimbalee ko Senegaal.\\n',\n",
       " 'ñaata at nga am',\n",
       " 'Abu Usmaan Si, Mamadu Yoro Jàllo ak Usmaan Njaay ñoo faatu, ci doxu nemmeeku koom-koom gi bu Maki Sàll bi.\\n',\n",
       " 'Ni ñu ko tàmm a waxe fii : boo xamatul foo jëm, dellul fa nga jóge woon, baax na lool ñu dellu fa ñu jóge woon ngir bégal way-jëfandiku yi.\\n',\n",
       " 'Zambie réew la mu bokk ci ONU, Union Africaine, ak Southern African Development Community (SADC).',\n",
       " 'tooy',\n",
       " 'Ay teemeeri nit faatu ca donu bu Nias, ci tefesu Sumatra.\\n']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wolof_corpus = raw_data[\"wolof\"]\n",
    "wolof_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9424569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_wolof_corpus, words_freq = utils.simple_tokenize(corpus=wolof_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1f4ed18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bataaxal',\n",
       "  'bii',\n",
       "  'jëwriñu',\n",
       "  'lu',\n",
       "  'ajju',\n",
       "  'ci',\n",
       "  'mbiru',\n",
       "  'bitim',\n",
       "  'réew',\n",
       "  'bu',\n",
       "  'ekuwatër',\n",
       "  'moo',\n",
       "  'ko',\n",
       "  'wara',\n",
       "  'wóoral',\n",
       "  'te',\n",
       "  'dafa',\n",
       "  'wara',\n",
       "  'mengoo',\n",
       "  'ak',\n",
       "  'yenni',\n",
       "  'càkuteef',\n",
       "  'yi'],\n",
       " ['amuñu', 'woon', 'benn', 'jot', 'ngir', 'rawal', 'sunu', 'bopp'],\n",
       " ['ekost', 'sa', 'dëkk', 'la', 'te', 'bëggna', 'nu', 'nga', 'toog', 'fi'],\n",
       " ['xibaari',\n",
       "  'jotna',\n",
       "  'espaañ',\n",
       "  'joxe',\n",
       "  'na',\n",
       "  'juróom',\n",
       "  'ñett',\n",
       "  'fukki',\n",
       "  'milyaar',\n",
       "  'ak',\n",
       "  'ñeent',\n",
       "  'ci',\n",
       "  'xaalisu',\n",
       "  'seefa',\n",
       "  'ngir',\n",
       "  'dimbalee',\n",
       "  'ko',\n",
       "  'senegaal'],\n",
       " ['ñaata', 'at', 'nga', 'am'],\n",
       " ['abu',\n",
       "  'usmaan',\n",
       "  'si',\n",
       "  'mamadu',\n",
       "  'yoro',\n",
       "  'jàllo',\n",
       "  'ak',\n",
       "  'usmaan',\n",
       "  'njaay',\n",
       "  'ñoo',\n",
       "  'faatu',\n",
       "  'ci',\n",
       "  'doxu',\n",
       "  'nemmeeku',\n",
       "  'koom',\n",
       "  'koom',\n",
       "  'gi',\n",
       "  'bu',\n",
       "  'maki',\n",
       "  'sàll',\n",
       "  'bi'],\n",
       " ['ni',\n",
       "  'ñu',\n",
       "  'ko',\n",
       "  'tàmm',\n",
       "  'a',\n",
       "  'waxe',\n",
       "  'fii',\n",
       "  'boo',\n",
       "  'xamatul',\n",
       "  'foo',\n",
       "  'jëm',\n",
       "  'dellul',\n",
       "  'fa',\n",
       "  'nga',\n",
       "  'jóge',\n",
       "  'woon',\n",
       "  'baax',\n",
       "  'na',\n",
       "  'lool',\n",
       "  'ñu',\n",
       "  'dellu',\n",
       "  'fa',\n",
       "  'ñu',\n",
       "  'jóge',\n",
       "  'woon',\n",
       "  'ngir',\n",
       "  'bégal',\n",
       "  'way',\n",
       "  'jëfandiku',\n",
       "  'yi'],\n",
       " ['zambie',\n",
       "  'réew',\n",
       "  'la',\n",
       "  'mu',\n",
       "  'bokk',\n",
       "  'ci',\n",
       "  'onu',\n",
       "  'union',\n",
       "  'africaine',\n",
       "  'ak',\n",
       "  'southern',\n",
       "  'african',\n",
       "  'development',\n",
       "  'community',\n",
       "  'sadc'],\n",
       " ['tooy'],\n",
       " ['ay',\n",
       "  'teemeeri',\n",
       "  'nit',\n",
       "  'faatu',\n",
       "  'ca',\n",
       "  'donu',\n",
       "  'bu',\n",
       "  'nias',\n",
       "  'ci',\n",
       "  'tefesu',\n",
       "  'sumatra']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_wolof_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38fda6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_model = markov_chains.MarkovChains(states=list(words_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae5526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25466,\n",
       " ['0',\n",
       "  '00',\n",
       "  '000',\n",
       "  '0006779',\n",
       "  '000ngir',\n",
       "  '007',\n",
       "  '01',\n",
       "  '012914',\n",
       "  '02',\n",
       "  '0230',\n",
       "  '036',\n",
       "  '04',\n",
       "  '043',\n",
       "  '05',\n",
       "  '06',\n",
       "  '07',\n",
       "  '07h19',\n",
       "  '08',\n",
       "  '0800',\n",
       "  '09',\n",
       "  '0ghz',\n",
       "  '1',\n",
       "  '10',\n",
       "  '100',\n",
       "  '1000',\n",
       "  '10000',\n",
       "  '100000',\n",
       "  '1000eelu',\n",
       "  '1002',\n",
       "  '100m',\n",
       "  '100ml',\n",
       "  '101',\n",
       "  '1014',\n",
       "  '103',\n",
       "  '104',\n",
       "  '1040',\n",
       "  '105',\n",
       "  '107',\n",
       "  '108',\n",
       "  '10w',\n",
       "  '10ème',\n",
       "  '11',\n",
       "  '110',\n",
       "  '1100',\n",
       "  '111',\n",
       "  '1127',\n",
       "  '114',\n",
       "  '116',\n",
       "  '117',\n",
       "  '118',\n",
       "  '119',\n",
       "  '11a',\n",
       "  '11b',\n",
       "  '11eel',\n",
       "  '11eelu',\n",
       "  '11g',\n",
       "  '11km',\n",
       "  '11milyoŋ',\n",
       "  '11n',\n",
       "  '12',\n",
       "  '120',\n",
       "  '1200',\n",
       "  '121i',\n",
       "  '122',\n",
       "  '1235',\n",
       "  '125',\n",
       "  '125m',\n",
       "  '12eel',\n",
       "  '12eelu',\n",
       "  '12reelu',\n",
       "  '13',\n",
       "  '130',\n",
       "  '1300',\n",
       "  '1310',\n",
       "  '1311',\n",
       "  '133',\n",
       "  '134',\n",
       "  '135',\n",
       "  '136',\n",
       "  '1375',\n",
       "  '139',\n",
       "  '13eelu',\n",
       "  '13½',\n",
       "  '14',\n",
       "  '140',\n",
       "  '1418',\n",
       "  '142',\n",
       "  '1444',\n",
       "  '1450',\n",
       "  '1451',\n",
       "  '1469',\n",
       "  '1480',\n",
       "  '149',\n",
       "  '1492',\n",
       "  '1498',\n",
       "  '14½',\n",
       "  '15',\n",
       "  '150',\n",
       "  '1500',\n",
       "  '150000',\n",
       "  '1537',\n",
       "  '1539',\n",
       "  '15eelu',\n",
       "  '15ème',\n",
       "  '16',\n",
       "  '160',\n",
       "  '160km',\n",
       "  '161',\n",
       "  '1610',\n",
       "  '1624',\n",
       "  '1639',\n",
       "  '164',\n",
       "  '1644',\n",
       "  '1649',\n",
       "  '165',\n",
       "  '166',\n",
       "  '167',\n",
       "  '168',\n",
       "  '1683',\n",
       "  '16eelu',\n",
       "  '16kg',\n",
       "  '16w',\n",
       "  '17',\n",
       "  '174',\n",
       "  '175',\n",
       "  '1755',\n",
       "  '1759',\n",
       "  '1767',\n",
       "  '177',\n",
       "  '1770',\n",
       "  '1772',\n",
       "  '1776',\n",
       "  '1789',\n",
       "  '17eelu',\n",
       "  '18',\n",
       "  '180',\n",
       "  '1800',\n",
       "  '1810',\n",
       "  '1813',\n",
       "  '1818',\n",
       "  '1819',\n",
       "  '1821',\n",
       "  '183',\n",
       "  '1830',\n",
       "  '1835',\n",
       "  '1839',\n",
       "  '1850',\n",
       "  '1861',\n",
       "  '1870',\n",
       "  '1875',\n",
       "  '1878',\n",
       "  '1879',\n",
       "  '1880',\n",
       "  '1882',\n",
       "  '1884',\n",
       "  '1885',\n",
       "  '1889',\n",
       "  '1894',\n",
       "  '1895',\n",
       "  '1898',\n",
       "  '18eelu',\n",
       "  '18i',\n",
       "  '19',\n",
       "  '190',\n",
       "  '1900',\n",
       "  '1905',\n",
       "  '190eelu',\n",
       "  '1910',\n",
       "  '1912',\n",
       "  '1915',\n",
       "  '1917',\n",
       "  '1918',\n",
       "  '1920',\n",
       "  '1920s',\n",
       "  '1921',\n",
       "  '1922',\n",
       "  '1922siwal',\n",
       "  '1928',\n",
       "  '1930',\n",
       "  '1935',\n",
       "  '1936',\n",
       "  '1937',\n",
       "  '1938',\n",
       "  '1939',\n",
       "  '1940',\n",
       "  '1942',\n",
       "  '1945',\n",
       "  '1947',\n",
       "  '1949',\n",
       "  '1950',\n",
       "  '1951',\n",
       "  '1952',\n",
       "  '1953',\n",
       "  '1956',\n",
       "  '1957',\n",
       "  '1958',\n",
       "  '1959',\n",
       "  '1960',\n",
       "  '1961',\n",
       "  '1962',\n",
       "  '1962ba',\n",
       "  '1963',\n",
       "  '1964',\n",
       "  '1965',\n",
       "  '1966',\n",
       "  '1967',\n",
       "  '1968',\n",
       "  '1969',\n",
       "  '1970',\n",
       "  '1972',\n",
       "  '1973',\n",
       "  '1974',\n",
       "  '1975',\n",
       "  '1976',\n",
       "  '1977',\n",
       "  '1978',\n",
       "  '1979',\n",
       "  '1980',\n",
       "  '1980s',\n",
       "  '1981',\n",
       "  '1982',\n",
       "  '1983',\n",
       "  '1984',\n",
       "  '1985',\n",
       "  '1986',\n",
       "  '1987',\n",
       "  '1988',\n",
       "  '1989',\n",
       "  '1990',\n",
       "  '1992',\n",
       "  '1993',\n",
       "  '1994',\n",
       "  '1995',\n",
       "  '1996',\n",
       "  '1997',\n",
       "  '1998',\n",
       "  '1999',\n",
       "  '19eel',\n",
       "  '19h',\n",
       "  '1kg',\n",
       "  '1u',\n",
       "  '2',\n",
       "  '20',\n",
       "  '200',\n",
       "  '2000',\n",
       "  '200000',\n",
       "  '2001',\n",
       "  '2002',\n",
       "  '2003',\n",
       "  '2004',\n",
       "  '2005',\n",
       "  '2006',\n",
       "  '2007',\n",
       "  '2008',\n",
       "  '2009',\n",
       "  '2010',\n",
       "  '2011',\n",
       "  '2012',\n",
       "  '2013',\n",
       "  '2014',\n",
       "  '2015',\n",
       "  '2016',\n",
       "  '2017',\n",
       "  '2018',\n",
       "  '2019',\n",
       "  '2019cai',\n",
       "  '2020',\n",
       "  '2021',\n",
       "  '2022',\n",
       "  '2023',\n",
       "  '2024',\n",
       "  '2025',\n",
       "  '2026',\n",
       "  '202i',\n",
       "  '2030',\n",
       "  '2035',\n",
       "  '20407',\n",
       "  '2045',\n",
       "  '2050',\n",
       "  '206',\n",
       "  '207',\n",
       "  '208',\n",
       "  '20865',\n",
       "  '20e',\n",
       "  '20eelu',\n",
       "  '20h30',\n",
       "  '20m',\n",
       "  '21',\n",
       "  '21h19',\n",
       "  '22',\n",
       "  '220',\n",
       "  '221',\n",
       "  '2250',\n",
       "  '226',\n",
       "  '22eelu',\n",
       "  '23',\n",
       "  '230',\n",
       "  '235',\n",
       "  '24',\n",
       "  '240',\n",
       "  '243',\n",
       "  '247',\n",
       "  '24eelu',\n",
       "  '24mm',\n",
       "  '24½',\n",
       "  '25',\n",
       "  '250',\n",
       "  '25000',\n",
       "  '257',\n",
       "  '26',\n",
       "  '262',\n",
       "  '264',\n",
       "  '27',\n",
       "  '270',\n",
       "  '2706',\n",
       "  '28',\n",
       "  '281',\n",
       "  '29',\n",
       "  '291',\n",
       "  '292',\n",
       "  '293',\n",
       "  '299',\n",
       "  '29¾',\n",
       "  '2stv',\n",
       "  '3',\n",
       "  '30',\n",
       "  '300',\n",
       "  '3000',\n",
       "  '30000',\n",
       "  '300000',\n",
       "  '30min',\n",
       "  '31',\n",
       "  '310',\n",
       "  '3136',\n",
       "  '32',\n",
       "  '320',\n",
       "  '323',\n",
       "  '328',\n",
       "  '33',\n",
       "  '330',\n",
       "  '332',\n",
       "  '335',\n",
       "  '34',\n",
       "  '340',\n",
       "  '35',\n",
       "  '350',\n",
       "  '356',\n",
       "  '35milyoŋ',\n",
       "  '35mm',\n",
       "  '36',\n",
       "  '360',\n",
       "  '3600',\n",
       "  '360000',\n",
       "  '3680',\n",
       "  '36mm',\n",
       "  '37',\n",
       "  '375',\n",
       "  '378',\n",
       "  '379',\n",
       "  '37eelu',\n",
       "  '38',\n",
       "  '380',\n",
       "  '383',\n",
       "  '384',\n",
       "  '385',\n",
       "  '387km',\n",
       "  '39',\n",
       "  '390',\n",
       "  '396i',\n",
       "  '398',\n",
       "  '39c',\n",
       "  '3t',\n",
       "  '4',\n",
       "  '40',\n",
       "  '400',\n",
       "  '4000',\n",
       "  '40000',\n",
       "  '403',\n",
       "  '407',\n",
       "  '41',\n",
       "  '42',\n",
       "  '420',\n",
       "  '429',\n",
       "  '43',\n",
       "  '438',\n",
       "  '439',\n",
       "  '44',\n",
       "  '442',\n",
       "  '447',\n",
       "  '45',\n",
       "  '46',\n",
       "  '463',\n",
       "  '47',\n",
       "  '470',\n",
       "  '4700',\n",
       "  '48',\n",
       "  '480km',\n",
       "  '4892',\n",
       "  '49',\n",
       "  '491',\n",
       "  '497',\n",
       "  '4_',\n",
       "  '4g',\n",
       "  '4ghz',\n",
       "  '4x4',\n",
       "  '5',\n",
       "  '50',\n",
       "  '500',\n",
       "  '5000',\n",
       "  '500k',\n",
       "  '51',\n",
       "  '5129',\n",
       "  '52',\n",
       "  '53',\n",
       "  '54',\n",
       "  '540',\n",
       "  '55',\n",
       "  '550',\n",
       "  '555',\n",
       "  '56',\n",
       "  '562',\n",
       "  '563',\n",
       "  '57',\n",
       "  '589',\n",
       "  '596',\n",
       "  '599',\n",
       "  '5eelu',\n",
       "  '5i',\n",
       "  '5m',\n",
       "  '6',\n",
       "  '60',\n",
       "  '600',\n",
       "  '600mbit',\n",
       "  '60eel',\n",
       "  '61',\n",
       "  '618',\n",
       "  '62',\n",
       "  '623',\n",
       "  '63',\n",
       "  '6321',\n",
       "  '64',\n",
       "  '646',\n",
       "  '64kph',\n",
       "  '65',\n",
       "  '650',\n",
       "  '6500',\n",
       "  '66',\n",
       "  '67',\n",
       "  '673',\n",
       "  '68',\n",
       "  '687',\n",
       "  '688',\n",
       "  '69',\n",
       "  '694',\n",
       "  '6h',\n",
       "  '7',\n",
       "  '70',\n",
       "  '700',\n",
       "  '7000',\n",
       "  '708',\n",
       "  '70km',\n",
       "  '72',\n",
       "  '722',\n",
       "  '74',\n",
       "  '75',\n",
       "  '750',\n",
       "  '752',\n",
       "  '755',\n",
       "  '76',\n",
       "  '76000',\n",
       "  '764',\n",
       "  '76s',\n",
       "  '77',\n",
       "  '77000',\n",
       "  '773',\n",
       "  '78',\n",
       "  '783',\n",
       "  '787',\n",
       "  '7888',\n",
       "  '79',\n",
       "  '7eelu',\n",
       "  '7palaas',\n",
       "  '7th',\n",
       "  '7tv',\n",
       "  '8',\n",
       "  '80',\n",
       "  '800',\n",
       "  '800x600',\n",
       "  '802',\n",
       "  '809',\n",
       "  '80s',\n",
       "  '81',\n",
       "  '8154',\n",
       "  '820',\n",
       "  '820000',\n",
       "  '83',\n",
       "  '832',\n",
       "  '85',\n",
       "  '850km²',\n",
       "  '859',\n",
       "  '85mph',\n",
       "  '86',\n",
       "  '864',\n",
       "  '865',\n",
       "  '88',\n",
       "  '887',\n",
       "  '89',\n",
       "  '8bn',\n",
       "  '8eelu',\n",
       "  '9',\n",
       "  '90',\n",
       "  '900',\n",
       "  '90kg',\n",
       "  '92',\n",
       "  '93',\n",
       "  '94',\n",
       "  '9400',\n",
       "  '948',\n",
       "  '95',\n",
       "  '96',\n",
       "  '964',\n",
       "  '97',\n",
       "  '974',\n",
       "  '980',\n",
       "  '99',\n",
       "  '991',\n",
       "  '996',\n",
       "  '999',\n",
       "  '9h',\n",
       "  '9h30',\n",
       "  '9w30',\n",
       "  '_nun',\n",
       "  'a',\n",
       "  'a1',\n",
       "  'a1gp',\n",
       "  'a320',\n",
       "  'a391',\n",
       "  'aa',\n",
       "  'aaada',\n",
       "  'aab',\n",
       "  'aada',\n",
       "  'aadaa',\n",
       "  'aadaak',\n",
       "  'aadaam',\n",
       "  'aadam',\n",
       "  'aadama',\n",
       "  'aaday',\n",
       "  'aadiya',\n",
       "  'aafal',\n",
       "  'aafiya',\n",
       "  'aagibu',\n",
       "  'aah',\n",
       "  'aaj',\n",
       "  'aajo',\n",
       "  'aajoowuton',\n",
       "  'aajowoo',\n",
       "  'aajoy',\n",
       "  'aak',\n",
       "  'aakalaj',\n",
       "  'aakimo',\n",
       "  'aakimoo',\n",
       "  'aalam',\n",
       "  'aali',\n",
       "  'aalu',\n",
       "  'aalug',\n",
       "  'aamadu',\n",
       "  'aamerig',\n",
       "  'aaml',\n",
       "  'aan',\n",
       "  'aana',\n",
       "  'aandnañu',\n",
       "  'aantu',\n",
       "  'aapia',\n",
       "  'aar',\n",
       "  'aaraffu',\n",
       "  'aaram',\n",
       "  'aarasma',\n",
       "  'aare',\n",
       "  'aaree',\n",
       "  'aareen',\n",
       "  'aarkaat',\n",
       "  'aarkat',\n",
       "  'aaron',\n",
       "  'aart',\n",
       "  'aartu',\n",
       "  'aaru',\n",
       "  'aarësistere',\n",
       "  'aaskanu',\n",
       "  'aaw',\n",
       "  'aay',\n",
       "  'aaye',\n",
       "  'aayeg',\n",
       "  'aayul',\n",
       "  'ab',\n",
       "  'abaabakar',\n",
       "  'abaas',\n",
       "  'abadan',\n",
       "  'abajada',\n",
       "  'abal',\n",
       "  'abale',\n",
       "  'abaloon',\n",
       "  'abbaas',\n",
       "  'abbu',\n",
       "  'abc',\n",
       "  'abdu',\n",
       "  'abdul',\n",
       "  'abdulaay',\n",
       "  'abduraxmaan',\n",
       "  'abe',\n",
       "  'abedi',\n",
       "  'abel',\n",
       "  'abercrombie',\n",
       "  'aberkane',\n",
       "  'abib',\n",
       "  'abigail',\n",
       "  'abiib',\n",
       "  'abijaŋ',\n",
       "  'abijãa',\n",
       "  'abiyoŋ',\n",
       "  'abiyoŋu',\n",
       "  'ablaay',\n",
       "  'able',\n",
       "  'abone',\n",
       "  'aboneel',\n",
       "  'abonmaa',\n",
       "  'abosolutism',\n",
       "  'abraham',\n",
       "  'absa',\n",
       "  'absirdistaa',\n",
       "  'abu',\n",
       "  'abuja',\n",
       "  'abujaa',\n",
       "  'abulaay',\n",
       "  'abutsaa',\n",
       "  'abuubakar',\n",
       "  'ac',\n",
       "  'academi',\n",
       "  'academy',\n",
       "  'accident',\n",
       "  'accord',\n",
       "  'aceh',\n",
       "  'acetaminophen',\n",
       "  'acide',\n",
       "  'ackland',\n",
       "  'acma',\n",
       "  'acrylic',\n",
       "  'acrylik',\n",
       "  'act',\n",
       "  'acta',\n",
       "  'acteur',\n",
       "  'acting',\n",
       "  'action',\n",
       "  'actrice',\n",
       "  'actu',\n",
       "  'ad',\n",
       "  'adam',\n",
       "  'adama',\n",
       "  'adams',\n",
       "  'adapter',\n",
       "  'adarees',\n",
       "  'adareesu',\n",
       "  'add',\n",
       "  'addenbrooke',\n",
       "  'addiina',\n",
       "  'addina',\n",
       "  'addo',\n",
       "  'addu',\n",
       "  'addukalpe',\n",
       "  'adduna',\n",
       "  'adekoya',\n",
       "  'adel',\n",
       "  'adelaïde',\n",
       "  'adicomdays',\n",
       "  'adidas',\n",
       "  'adie',\n",
       "  'adien',\n",
       "  'adina',\n",
       "  'aditi',\n",
       "  'adjektif',\n",
       "  'adk',\n",
       "  'admin',\n",
       "  'administrasoŋ',\n",
       "  'adn',\n",
       "  'adnistarasiyoŋ',\n",
       "  'adnu',\n",
       "  'ado',\n",
       "  'adonis',\n",
       "  'adopsiyong',\n",
       "  'adopte',\n",
       "  'adoptee',\n",
       "  'adres',\n",
       "  'adresu',\n",
       "  'adriftmorn',\n",
       "  'adsektif',\n",
       "  'adt',\n",
       "  'aduna',\n",
       "  'adunam',\n",
       "  'advocaat',\n",
       "  'adweerb',\n",
       "  'aeopor',\n",
       "  'aerobic',\n",
       "  'aeropoor',\n",
       "  'aeroport',\n",
       "  'aerosmith',\n",
       "  'afcfta',\n",
       "  'afeer',\n",
       "  'afeeri',\n",
       "  'affer',\n",
       "  'affordable',\n",
       "  'afganistaa',\n",
       "  'afganistaan',\n",
       "  'afganistan',\n",
       "  'afghan',\n",
       "  'afghanistan',\n",
       "  'afghans',\n",
       "  'afghanstan',\n",
       "  'afiis',\n",
       "  'afirig',\n",
       "  'afisu',\n",
       "  'afp',\n",
       "  'africa',\n",
       "  'africain',\n",
       "  'africaine',\n",
       "  'african',\n",
       "  'afrig',\n",
       "  'afrigam',\n",
       "  'afrigu',\n",
       "  'afrik',\n",
       "  'afrique',\n",
       "  'afro',\n",
       "  'afrobasket',\n",
       "  'ag',\n",
       "  'agder',\n",
       "  'age',\n",
       "  'agence',\n",
       "  'agent',\n",
       "  'agero',\n",
       "  'ageroo',\n",
       "  'ages',\n",
       "  'agetip',\n",
       "  'agg',\n",
       "  'aggale',\n",
       "  'agge',\n",
       "  'aggression',\n",
       "  'aggsee',\n",
       "  'agguñu',\n",
       "  'agir',\n",
       "  'agneau',\n",
       "  'agnostic',\n",
       "  'ago',\n",
       "  'agon',\n",
       "  'agregateur',\n",
       "  'agro',\n",
       "  'agsee',\n",
       "  'agseeg',\n",
       "  'agsi',\n",
       "  'agsiwoon',\n",
       "  'agsiwul',\n",
       "  'aguero',\n",
       "  'aguerro',\n",
       "  'agung',\n",
       "  'aguwero',\n",
       "  'aguñu',\n",
       "  'ah',\n",
       "  'ahmadu',\n",
       "  'ahmat',\n",
       "  'ahmeyim',\n",
       "  'ahã',\n",
       "  'ai',\n",
       "  'aibd',\n",
       "  'aida',\n",
       "  'aifrig',\n",
       "  'aigles',\n",
       "  'air',\n",
       "  'airbus',\n",
       "  'aires',\n",
       "  'airlines',\n",
       "  'airplane',\n",
       "  'airport',\n",
       "  'airways',\n",
       "  'aissa',\n",
       "  'aitutaki',\n",
       "  'aix',\n",
       "  'aj',\n",
       "  'ajaa',\n",
       "  'ajaccio',\n",
       "  'ajacides',\n",
       "  'ajana',\n",
       "  'ajandeem',\n",
       "  'ajandi',\n",
       "  'ajandiko',\n",
       "  'ajandina',\n",
       "  'ajandiwaat',\n",
       "  'ajar',\n",
       "  'ajax',\n",
       "  'aji',\n",
       "  'ajj',\n",
       "  'ajju',\n",
       "  'ajjuma',\n",
       "  'ajkat',\n",
       "  'ajoo',\n",
       "  'ajoon',\n",
       "  'aju',\n",
       "  'ajubés',\n",
       "  'ajuwul',\n",
       "  'ajuwutoon',\n",
       "  'ak',\n",
       "  'aka',\n",
       "  'akaademi',\n",
       "  'akadami',\n",
       "  'akala',\n",
       "  'aki',\n",
       "  'akilee',\n",
       "  'akisdaa',\n",
       "  'akit',\n",
       "  'akiten',\n",
       "  'akk',\n",
       "  'akkademi',\n",
       "  'akordewon',\n",
       "  'akosua',\n",
       "  'aks',\n",
       "  'aksan',\n",
       "  'aksee',\n",
       "  'aksidaa',\n",
       "  'aksidang',\n",
       "  'aksidanté',\n",
       "  'aksidaŋ',\n",
       "  'aksiden',\n",
       "  'aksideng',\n",
       "  'aksidãa',\n",
       "  'aksina',\n",
       "  'aksioŋ',\n",
       "  'aksiyon',\n",
       "  'aksu',\n",
       "  'aksum',\n",
       "  'aksumite',\n",
       "  'aksé',\n",
       "  'akteer',\n",
       "  'aktiwism',\n",
       "  'aktiwist',\n",
       "  'aktiwité',\n",
       "  'aktris',\n",
       "  'aktër',\n",
       "  'akufo',\n",
       "  'akustik',\n",
       "  'al',\n",
       "  'alaaji',\n",
       "  'alaal',\n",
       "  'alaan',\n",
       "  'alaba',\n",
       "  'alabaa',\n",
       "  'alag',\n",
       "  'alain',\n",
       "  'alak',\n",
       "  'alal',\n",
       "  'alalam',\n",
       "  'alali',\n",
       "  'alalu',\n",
       "  'alam',\n",
       "  'alamaan',\n",
       "  'alaman',\n",
       "  'alan',\n",
       "  'alarba',\n",
       "  'alasaan',\n",
       "  'alasdair',\n",
       "  'alaska',\n",
       "  'alatereete',\n",
       "  'alawes',\n",
       "  'alax',\n",
       "  'alaxam',\n",
       "  'alaxu',\n",
       "  'alba',\n",
       "  'albeer',\n",
       "  'albence',\n",
       "  'albense',\n",
       "  'albert',\n",
       "  'albom',\n",
       "  'albuquerque',\n",
       "  'alden',\n",
       "  'aldi',\n",
       "  'aldwych',\n",
       "  'ale',\n",
       "  'aleisk',\n",
       "  'alejandro',\n",
       "  'aleksandr',\n",
       "  'alersi',\n",
       "  'alessandro',\n",
       "  'alex',\n",
       "  'alexander',\n",
       "  'alexandri',\n",
       "  'alexiou',\n",
       "  'aley',\n",
       "  'alfa',\n",
       "  'alfaa',\n",
       "  'alferet',\n",
       "  'alfonso',\n",
       "  'alfredo',\n",
       "  'alfret',\n",
       "  'algo',\n",
       "  'algue',\n",
       "  'algues',\n",
       "  'algérie',\n",
       "  'alhamdulilah',\n",
       "  'ali',\n",
       "  'alien',\n",
       "  'alikalaj',\n",
       "  'alimi',\n",
       "  'alimiñom',\n",
       "  'alin',\n",
       "  'alison',\n",
       "  'alisson',\n",
       "  'aliw',\n",
       "  'aliwu',\n",
       "  'aliwun',\n",
       "  'aliy',\n",
       "  'aliye',\n",
       "  'aliyeen',\n",
       "  'aliyu',\n",
       "  'aljana',\n",
       "  'aljanaay',\n",
       "  'aljuma',\n",
       "  'aljumz',\n",
       "  'alkaati',\n",
       "  'alkaline',\n",
       "  'alkami',\n",
       "  'alkamis',\n",
       "  'alkati',\n",
       "  'alkol',\n",
       "  'alkool',\n",
       "  'alku',\n",
       "  'all',\n",
       "  'allah',\n",
       "  'allal',\n",
       "  'allamaanu',\n",
       "  'allamagne',\n",
       "  'allan',\n",
       "  'allarba',\n",
       "  'allardyce',\n",
       "  'allee',\n",
       "  'allemagne',\n",
       "  'allemand',\n",
       "  'allen',\n",
       "  'alleva',\n",
       "  'alliance',\n",
       "  'allies',\n",
       "  'alliw',\n",
       "  'alloa',\n",
       "  'allu',\n",
       "  'alluk',\n",
       "  'allum',\n",
       "  'alluwa',\n",
       "  'alluway',\n",
       "  'allëmaaañ',\n",
       "  'almaa',\n",
       "  'almaan',\n",
       "  'almaañ',\n",
       "  'almadi',\n",
       "  'almanac',\n",
       "  'almañ',\n",
       "  'almet',\n",
       "  'almãa',\n",
       "  'alonso',\n",
       "  'alpes',\n",
       "  'alpha',\n",
       "  'alphabet',\n",
       "  'alphonse',\n",
       "  'alpin',\n",
       "  'alpinism',\n",
       "  'alpulaar',\n",
       "  'alpë',\n",
       "  'alsar',\n",
       "  'alsee',\n",
       "  'alseeri',\n",
       "  'alseri',\n",
       "  'alteer',\n",
       "  'altine',\n",
       "  'altiné',\n",
       "  'altoppeem',\n",
       "  'alumet',\n",
       "  'alves',\n",
       "  'alxamas',\n",
       "  'alxames',\n",
       "  'alxamess',\n",
       "  'alxamis',\n",
       "  'alxemes',\n",
       "  'alxuraan',\n",
       "  'alxuraanu',\n",
       "  'alxuwaatna',\n",
       "  'alëma',\n",
       "  'alëmaa',\n",
       "  'alëmaañ',\n",
       "  'am',\n",
       "  'ama',\n",
       "  'amaa',\n",
       "  'amaale',\n",
       "  'amaana',\n",
       "  'amaanna',\n",
       "  'amaat',\n",
       "  'amaatna',\n",
       "  'amaatëer',\n",
       "  ...])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(markov_model.states), markov_model.states # I need a better tokenization function; more to come"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09152a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tfidf: 2900it [00:06, 457.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tf_idf \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mtfidf(corpus\u001b[38;5;241m=\u001b[39mwolof_corpus)\n",
      "File \u001b[0;32m~/Desktop/machine-learning-grind/11-nlp/02-language-modeling/utils.py\u001b[0m, in \u001b[0;36mtfidf\u001b[0;34m(corpus, vocab_size, tokenize_function)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tf_idf = utils.tfidf(corpus=wolof_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f12a8d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indi nañu lépp lu ci war ci li weesu wuute ak sàrt bii teew te jóge ci Kilifay nguur gi wala yi yor wàllu galag, te ñu mën see téye yii nekk ca : bu limat 0006779/MEF/DGID/BLEC bu ñaar-fukki fan ci weeru ut 2004 ; xibaar yi, bataaxal yi ak tontu waa nguur gi te jóge ca kër jëwriñ ja yor wàllu koom ak koppaaral ak barab bu mag bay doxal mbirum galag ak këyit yi ci aju.\n",
      " Sont rapportées toutes dispositions réglementaires antérieures contraires à la présente loi émanant des autorités administratives ou fiscales, notamment celles contenues dans: la circulaire n° 0006779/MEF/DGID/BLEC du 20 août 2004; les circulaires, notes, lettres et réponses administratives émanant du Ministère de l’Economie et des finances et de la Direction générale des Impôts et des Domaines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ex, l in zip(wolof_corpus, raw_data[\"french\"]) :\n",
    "    if \"0006779\" in ex.lower():\n",
    "        print(ex, l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
