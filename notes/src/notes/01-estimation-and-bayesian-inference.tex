\documentclass[12pt, a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{tikz}
\usepackage{bbm}
\usepackage{hyperref}
\usepackage[authoryear]{natbib}
\usepackage{url}
\usepackage{geometry}
\usepackage{keyval}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{enumitem}
\usepackage{float}
\usepackage[table]{xcolor}
\usepackage{makecell}
\usepackage[none]{hyphenat}
\usepackage{amsfonts}
\usepackage{algpseudocode}
\usetikzlibrary{positioning}
\usepackage{cancel}
\usepackage{blkarray}
\usepackage{multirow}
\usepackage{fancyhdr}
\usepackage[absolute,overlay]{textpos}

% Page Layout
\geometry{margin=1in}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage}

\renewcommand*{\sectionmark}[1]{\markright{\thesection.~~#1}}
\renewcommand*{\subsectionmark}[1]{\markright{\thesubsection.~~#1}}
\renewcommand*{\subsubsectionmark}[1]{\markright{\thesubsubsection.~~#1}}
\lhead{\sffamily \rightmark}
\renewcommand{\headrulewidth}{0pt}

% use dots instead of -
\AtBeginDocument{
  \def\labelitemi{$\bullet$}
}
\sloppy

% keys & commands
\input{src/keys.tex}
\input{src/commands.tex}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

% Define unnumbered environments
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem*{remark}{Remark} % No numbering
\setlength{\parindent}{0pt} % remove indent
\numberwithin{figure}{section}
\numberwithin{equation}{section}
\numberwithin{table}{section}

\author{Baye Malick Gning}
\date{}

% TITLE
\title{Estimation and Bayesian inference}


\begin{document}

\maketitle
\vspace{2cm}


\newpage
\tableofcontents
\newpage


% BEGIN
\section{Introduction}
We explore the process of defining a model family as a type of joint distribution $P(X)=P(X_1,...,X_m)$. It then discusses how to use the model for prediction and sampling. Finally, it addresses the methods for learning and estimating model parameters from data, focusing on techniques such as the Maximum Likelihood Estimator (MLE) and Bayesian inference.

\section{Define a model}
The first step in building a statistical model is to make assumptions about the data. These assumptions allow the model to represent the system effectively.

\subsection{Discriminative and generative models}
Statistical models are often categorized as either discriminative or generative.

\begin{definition}[Discriminative model]
Discriminative models focus on predicting an output variable $Y$ given input variables $X=(X_1, X_2)$. For example, in linear regression, the relationship is modeled as $Y = c_0 + c_1 X_1 + c_2 X_2 + \epsilon$, where $\epsilon$ is Gaussian noise. While discriminative models excel at prediction, they do not model the distribution of $X$, making simulation impossible.
\end{definition}

\begin{definition}[Generative model]
Generative models aim to model the joint distribution $P(X, Y)$, allowing the generation of both input and output samples. Generative models encompass discriminative models by marginalizing over inputs, as shown in equation \ref{eq:generative_model}. However, they are restrictive as they require to model the input distribution.

\begin{equation}
    p(Y | X, \theta) = \frac{p(X, Y | \theta)}{p(X | \theta)} = \frac{p(X, Y | \theta)}{\int p_{(X,Y)}(X,Y=y|\theta)dy}
    \label{eq:generative_model}
\end{equation}

\end{definition}

\section{Using model for prediction}
Prediction involves estimating the output $Y$ given input $X$ and model parameters $\theta$. There are two main approaches:
\begin{itemize}
    \item point estimation assumes that $\theta$ is fixed, leading to predictions based on $P(Y|X)=P(Y|X,\theta=\hat \theta)$.
    \item Bayesian inference considers $\theta$ as uncertain, represented by a posterior distribution modeled by $\Theta \sim p(\theta|\kappa)$. $\kappa$ are hyperparameters and $P(Y|X,\kappa)$ is the posterior predictive distribution.
\end{itemize}
Point estimation is considered a weak method because it provides a single, fixed value without accounting for uncertainty, whereas Bayesian inference is considered a strong one because it incorporates prior knowledge and updates it with data to produce a distribution, which captures both the estimate and the associated uncertainty.

\section{Learning a Model}
Learning a statistical model involves estimating its parameters from data.

\subsection{Bayes' theorem}
Bayes' theorem is a fundamental result in probability theory that describes how to update beliefs in light of new evidence. It is derived by combining the definition of conditional probability with the product and sum rules. Formally, it is expressed as:

\begin{align}
p(X = x | Y = y) = \frac{p(X = x) p(Y = y | X = x)}{p(Y = y)}
\end{align}

Since the denominator represents the total probability of $Y = y$ across all possible values of $X$, it can be expanded using the law of total probability:

\begin{align}
p(X = x | Y = y) = \frac{p(X = x) p(Y = y | X = x)}{\sum\limits_{x'} p(X = x') p(Y = y | X = x')}
\end{align}

This theorem provides a systematic way to update prior probabilities, $p(X)$, with observed evidence, $Y = y$, through the likelihood $p(Y = y | X = x)$. The denominator ensures proper normalization by summing over all possible values of $X$.

Bayes' theorem plays a central role in Bayesian inference, allowing for the computation of posterior distributions based on prior knowledge and observed data. It is widely used in statistical modeling, decision theory, and machine learning applications such as classification and probabilistic reasoning.

\subsection{Heart of Bayesian inference}
As we treat parameters $\theta$ as unknown and data $\mathcal{D}$ fixed and known, we represent our uncertainty about parameters, after (posterior to) seeeing data, by computing the \textbf{posterior distribution} using Bayes' rule:

% p
\begin{align}
    p(\theta | \mathcal{D}) = \frac{p(\theta) p(\mathcal{D} | \theta)}{p(\mathcal{D})} = \frac{p(\theta) p(\mathcal{D} | \theta)}{\int_{\boldsymbol{\theta^{'}}} p(\mathcal{D}|\boldsymbol{\theta^{'}})p(\boldsymbol{\theta^{'}})}
\end{align}

where $p(\theta)$ is the \textbf{prior} and represents our beliefs about the parameters before seeing the data; $p(\mathcal{D} | \theta)$ is called the \textbf{likelihood}, and represents our beliefs about what data we except to see for each setting of the parameters; $p(\theta | \mathcal{D})$ is called the \textbf{posterior}, and represents our beliefs about the parameters after seeing the data; and $p(\mathcal{D})$ is called the \textbf{marginal likelihood} or \textbf{evidence}, and is a normalization constant.
The task of computing this posterior is called \textbf{Bayesian inference}, \textbf{posterior inference}, or just \textbf{inference}.

For likelihood, we assume the data are \textbf{iid} or \textbf{independent and identically} distributed. For prior, we assume we know nothing about the parameter. We note $p(\theta) = p(\theta|\kappa)$, with $\kappa$ called \textbf{hyperparameters} since they are parameters of the prior which determine our beliefs about the \textbf{main} parameter $\theta$.

\begin{remark}
\leavevmode
\begin{itemize}
\item Likelihood is not a distribution. It measures the relative plausibility of parameters given data.
\item i.i.d does not mean samples are independent. In practice, hidden dependencies may exist due to underlying structures or data collection methods.
\end{itemize}
\end{remark}

% \subsection{Bayesian concept learning}
% Mathematically, concept learning can be framed as a binary classification problem, where a function $f(x)$ determines whether an example $x$ belongs to a concept $C$ or not. Unlike traditional classification methods that require both positive and negative examples, Bayesian learning allows concepts to be inferred solely from positive examples by modeling uncertainty in the hypothesis space.
%
% A key aspect of Bayesian concept learning is the use of a hypothesis space of concepts, $\mathcal{H}$, which consists of potential explanations for the observed data. Given a dataset $D$ of positive examples, the set of hypotheses consistent with $D$ is known as the \textbf{version space}. As more examples are observed, the version space shrinks, leading to greater certainty about the correct concept.
%
% However, merely considering consistency with the data is insufficient to determine the best hypothesis. The Bayesian approach incorporates two additional principles: the \textbf{likelihood} and the \textbf{prior}.
%
% \boldtitle{Likelihood}
% The likelihood function $p(\mathcal{D}|h)$ measures how probable the observed data is under a given hypothesis $h$. A fundamental insight, known as the \textbf{size principle}, suggests that smaller, more specific hypotheses are preferred because observing only a subset of data that aligns with a broader hypothesis would be an unlikely coincidence.
%
% The prior $p(h)$ captures the inherent plausibility of a hypothesis before observing any data. Certain hypotheses, especially those that seem unnatural or overly specific, are assigned lower prior probabilities. This reflects the intuition that simpler, more generalizable concepts are preferable. The final step in Bayesian concept learning is computing the \textbf{posterior distribution}:
%
% \[
% p(h|D) = \frac{p(D|h) p(h)}{\sum_{h' \in H} p(D|h') p(h')}
% \]
%
% This posterior combines both the likelihood and the prior to determine the most probable hypothesis given the observed data. As more data is collected, the posterior distribution becomes increasingly peaked, ultimately converging to the Maximum A Posteriori (MAP) estimate. In the limit of infinite data, the MAP estimate approaches the Maximum Likelihood Estimate (MLE), meaning that the influence of the prior diminishes.
%
% One of the strengths of Bayesian learning is its ability to incorporate prior knowledge, allowing for rapid learning from limited data. However, the subjectivity of the prior remains a topic of debate, as different learners may arrive at different conclusions based on their prior assumptions. Despite this, Bayesian inference remains a powerful tool for modeling concept acquisition and generalization in both humans and machines.
%


\subsection{Maximum likelihood estimator (MLE)}
The principle of \textbf{maximum likelihood estimation} (MLE) assumes that the most plausible values for $\theta$ are those that maximize the probability of observing the given sample. Given a statistical model and observed data, the likelihood function quantifies how probable the observed data is for different values of the model parameters.

MLE estimates parameters by maximizing the likelihood of the observed data.
Given independent observations $\mathcal{D} = \{(x_i, y_i)\}_{1 \leq i \leq n}$, the likelihood function is given by:
\begin{equation}
\mathcal{L}(\theta; \mathcal{D}) = \prod\limits_{i=1}^n p(x_i, y_i | \theta)
\end{equation}

MLE is:
\begin{equation}
\hat{\theta}_{MLE} = \arg\max\limits_{\theta} L(\theta; \mathcal{D})
\end{equation}

where $p(x_i, y_i | \theta)$ represents the probability density (or mass) function for each observation under parameter $\theta$.


\begin{example}[A problem of network server]
\label{ex:network_problem}
A network server processes queries and sends back an answer message after some time. The size (in bytes) of the answer to the query has a mean and standard deviation which are affine functions of the size of the query (in bytes). The processing time average and standard deviation are proportional to the product of the sizes of the query and of the answer (i.e. quadratic complexity).\\

\boldtitle{Estimation of MLE}

Server's model is $T\sim \mathcal{N}(\mu,\sigma ^2)$ with $\theta=(\mu,\sigma)$. We note $\mathcal{D}=(t_i)_{1 \leq i \leq n}$ the observations. This leads to :

\begin{align}
\mathcal{L}(\theta; \mathcal{D})
&= \prod\limits_{i=1}^n p(t_i|\theta)=\prod\limits_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma}}e^{-\frac{(t_i-\mu)^2}{2 \sigma^2}}\\
&=\frac{1}{{\sqrt{2 \pi \sigma^2}}^n}e^{-\frac{\sum_{i=1}^n(t_i-\mu)^2}{2 \sigma^2}}
\end{align}
As $log$ is non-decreasing we can use it before deriving it. This gives:
\begin{align}
\log \mathcal{L}(\theta; \mathcal{D})
&=\frac{n}{2}\log 2\pi + n\log \sigma - \frac{1}{2\sigma^2}\sum_{i=1}^n(t_i-\mu)^2
\end{align}

\begin{align}
\nabla_{\theta} \log \mathcal{L}(\theta; \mathcal{D}) = 0
&\Leftrightarrow
\left\{
\begin{array}{l}
\frac{\partial \log \mathcal{L}(\theta; \mathcal{D})}{\partial \mu} = 0 \\ [1em]
\frac{\partial \log \mathcal{L}(\theta; \mathcal{D})}{\partial \sigma} = 0
\end{array}
\right.
\\
&\Leftrightarrow
\left\{
\begin{array}{l}
- \frac{1}{2\sigma^2}\sum_{i=1}^n(t_i-\mu) = 0 \\ [1em]
- \frac{n}{\sigma}-\frac{1}{\sigma^{-3}}\sum_{i=1}^n(t_i-\mu)^2
\end{array}
\right.
\\
&\Leftrightarrow
\left\{
\begin{array}{l}
    \hat\mu=\frac{1}{n}\sum_{i=1}^n t_i \\ [1em]
    \hat\sigma=\sqrt{\frac{\sum_{i=1}^n(t_i-\hat\mu)^2}{n}}
\end{array}
\right.
\end{align}
\end{example}


\subsection{Bayesian inference}
Bayesian inference offers several advantages, including the integration of prior knowledge and the ability to quantify uncertainty. For example, in a network server problem where processing times depend on query size, Bayesian methods can model uncertainties in both processing times and query sizes. However, practical challenges such as selecting priors and computational complexity must be addressed. Common solutions include using conjugate priors for analytical tractability or applying numerical methods like Markov Chain Monte Carlo (MCMC).

\begin{example}{Tossing coins (from \textbf{Probabilistic ML - Advanced Topics})}
Let $\theta \in [0,1]$ be the chance that some coin comes up heads, an event we denote by $Y = 1$. Suppose we toss a coin $N$ times, and we record the outcomes as $\mathcal{D} = \{y_n \in \{0, 1\} : n = 1 : N \}$. We want to compute $p(\theta|\mathcal{D})$, which represents our beliefs about the parameter after doing collecting the data. To compute the posterior, we can use Bayes’ rule.\\

The likelihood has the form:
\begin{equation}
    p(\mathcal{D}|\theta) = \prod_{n=1}^{N} \theta^{y_n} (1 - \theta)^{1 - y_n} = \theta^{N_1} (1 - \theta)^{N_0}=Beta(\theta|N_1+1, N_0+1)
\end{equation}

where we have defined $N_1 = \sum_{n=1}^{N} \mathbbm{1} (y_n = 1)$ and $N_0 = \sum_{n=1}^{N} \mathbbm{1} (y_n = 0)$, representing the number of heads and tails. These counts are called the \textbf{sufficient statistics} of the data, since this is all we need to know about $\mathcal{D}$ to infer $\theta$. The total count, $N = N_0 + N_1$, is called the \textbf{sample size}.\\

% Note that we can also consider a Binomial likelihood model, in which we perform $N$ trials and observe the number of heads, $y$, rather than observing a sequence of coin tosses. Now the likelihood has the following form:
% \begin{equation}
% p(D|\theta) = \text{Bin}(y|N, \theta) = \binom{N}{y} \theta^y (1 - \theta)^{N - y}
% \end{equation}
% The scaling factor $\binom{N}{y}$ is independent of $\theta$, so we can ignore it. Thus, this likelihood is proportional to the Bernoulli likelihood in Equation (1), so our inferences about $\theta$ will be the same for both models.
Let us assume we know nothing about the parameter, except that it lies in the interval $[0, 1]$. We can represent this uninformative prior using a uniform distribution:

\begin{equation}
    p(\theta) = \text{Unif}(\theta | 0, 1) = Beta(\theta|1,1)
\end{equation}

More generally, we will write the prior using a \textbf{beta distribution}.\\
% That is, we assume:
%
% \begin{equation}
% p(\theta) = \text{Beta}(\theta | \alpha, \beta) \propto \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}
% \end{equation}
%
% Here, $\alpha$ and $\beta$ are called \textbf{hyper-parameters}, since they are parameters of the prior which determine our beliefs about the “main” parameter $\theta$. If we set $\alpha = \beta = 1$, we recover the uniform prior as a special case.
%
% We can think of these hyper-parameters as \textbf{pseudocounts}, which play a role analogous to the empirical counts $N_1$ and $N_0$ derived from the real data. The strength of the prior is controlled by: $N = \alpha + \beta$. This is called the \textbf{equivalent sample size}, since it plays a role analogous to the observed sample size, $N = N_1 + N_0$.

We can compute the posterior by multiplying the likelihood by the prior:

\begin{equation}
p(\theta | \mathcal{D}) \propto \theta^{N_1} (1 - \theta)^{N_0} \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}
    \propto \text{Beta}(\theta | \alpha^{'} + N_1, \beta^{'} + N_0)
\end{equation}

where $\alpha^{'}=\alpha + N_1$ and $\beta^{'}=\beta + N_0$ are the parameters of the posterior. Since the posterior has the same functional form as the prior, we say that it is a \textbf{conjugate prior}.

For example, suppose we observe $N_1 = 4$ heads and $N_0 = 1$ tails. If we use a uniform prior, we get the posterior shown in Figure \ref{fig:bayesian-inference1}. Not surprisingly, this has exactly the same shape as the likelihood (but is scaled to integrate to 1 over the range $[0, 1]$).

\makeimage{
    path=images/01-bayesian-inference1.png,
    caption=Bayesian inference with,
    ref=bayesian-inference1,
    width=0.5
}{H}

Now suppose we use a prior that has a slight preference for values of $\theta$ near to $0.5$, reflecting our prior belief that it is more likely than not that the coin is fair. We will make this a weak prior by setting $\alpha = \beta = 2$. The effect of using this prior is illustrated in Figure \ref{fig:bayesian-inference2}. We see the posterior is a compromise between the prior and the likelihood.


\makeimage{
    path=images/02-bayesian-inference2.png,
    caption=Bayesian inference with,
    ref=bayesian-inference2,
    width=0.5
}{H}

Simulation is available \url{https://github.com/bmalick/machine-learning-grind/tree/master/00-stats/07-bayesian-inference}.

    % caption=Bayesian inference with $N_1=4$ and $N_0=1$ with $Uniform$ ($Beta(1,1)$) prior,

\end{example}

Bayesian inference with multiple samples has two properties:
\begin{itemize}
    \item processing of observations can be done one by and one and
    \item processing observations can be done in any order
\end{itemize}


\begin{align}
p(\theta | x_1,x_2) 
&\propto p(\theta) L(x_1,x_2 ; \theta)\\
&\propto p(\theta) L(x_1; \theta) L(x_2; \theta)\\
&\propto L(x_2; \theta) (L(x_1; \theta) p(\theta))=L(x_2; \theta) p(\theta|x_1)\\
&\propto L(x_1; \theta) (L(x_2; \theta) p(\theta))=L(x_1; \theta) p(\theta|x_2)\\
\end{align}

Bayesian inferance is naturally \textbf{online}.
Posterior contains all model information about past observations.

\begin{example}
Let's apply Bayesian inference in the case of example \ref{ex:network_problem}. Server's model is: $T \sim \mathcal{N}(\mu,\sigma^2)$ with $\theta=(\mu, \sigma)$. The prior is: $p(\theta) = \mathcal{N}(\mu; \mu_0, \sigma_0^2) \delta(\sigma - \sigma_T)$. Then $\kappa=(\mu_0,\sigma_0,\sigma_T)$ are the hyperparameters.\\

For one single observation $\mathcal{D}=\{t\}$, we derive the posterior and our goal is to find an expression that depends on $\theta=(\mu,\sigma)$:

\begin{align}
p(\theta | t) 
&\propto p(\theta) L(t; \theta)\\
&\propto \frac{1}{\sigma_0\sqrt{2 \pi}}e^{-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2}}\delta(\sigma - \sigma_T)\frac{1}{\sigma \sqrt{2 \pi}}e^{-\frac{(t-\mu)^2}{2 \sigma^2}}\\
&\propto e^{-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2}-\frac{(t-\mu)^2}{2 \sigma^2}}\delta(\sigma - \sigma_T)\\
&\propto e^{-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2}-\frac{(t-\mu)^2}{2 \sigma_T^2}}\delta(\sigma - \sigma_T)\\
&\propto e^{-\frac{(\mu-\mu_0)^2}{2 \sigma_0^2}-\frac{(t-\mu)^2}{2 \sigma_T^2}}\delta(\sigma - \sigma_T)\\
\end{align}

As,
% \underbrace{
\begin{align}
\frac{(t - \mu)^2}{\sigma_T^2} + \frac{(\mu - \mu_0)^2}{\sigma_0^2}
&=\frac{t^2}{\sigma_T^2} - \frac{2t\mu}{\sigma_T^2} + \frac{\mu^2}{\sigma_T^2} + \frac{\mu^2}{\sigma_0^2} - \frac{2 \mu \mu_0}{\sigma_0^2} + \frac{\mu_0^2}{\sigma_0^2}\\
    &= \mu^2 \underbrace{\left( \frac{1}{\sigma_T^2} + \frac{1}{\sigma_0^2} \right)}_{=\sigma_1^2} - 2\mu \underbrace{\left( \frac{t}{\sigma_T^2} + \frac{\mu_0}{\sigma_0^2} \right)}_{=\frac{\mu_1}{\sigma_1^2}} + \frac{t^2}{\sigma_T^2} + \frac{\mu_0^2}{\sigma_0^2}\\
&=\frac{\mu^2}{\sigma_1^2} - 2\mu \frac{\mu_1}{\sigma_1^2} + \frac{t^2}{\sigma_T^2} + \frac{\mu_0^2}{\sigma_0^2}\\
&= \frac{(\mu - \mu_1)^2}{\sigma_1^2} - \frac{\mu_1^2}{\sigma_1^2} + \frac{t^2}{\sigma_T^2} + \frac{\mu_0^2}{\sigma_0^2}
\end{align}

and:
\begin{align}
- \frac{\mu_1^2}{\sigma_1^2} + \frac{t^2}{\sigma_T^2} + \frac{\mu_0^2}{\sigma_0^2}
&= - \frac{\mu_1}{\sigma_1^2} + \frac{t^2}{\sigma_T^2} + \frac{\mu_0^2}{\sigma_0^2} - \left( \frac{1}{\sigma_0^2} + \frac{1}{\sigma_T^2} \right) \left( \frac{\mu_0}{\sigma_0^2} + \frac{t}{\sigma_T^2} \right)^2\\
&= - \frac{\mu_0^2 \sigma_T^2 + t^2 \sigma_0^2 + 2\mu_0 t \sigma_0^2}{\sigma_T^2 \sigma_0^2} + \frac{t^2}{\sigma_T^2} + \frac{\mu_0^2}{\sigma_0^2}\\
&= - \frac{\mu_0^2}{\sigma_0^2} - \frac{t^2}{\sigma_T^2} - 2\frac{\mu_0 t}{\sigma_T^2} + t^2 + \mu_0^2 + \frac{t^2 \sigma_0^2}{\sigma_T^2} + \frac{\mu_0^2 \sigma_T^2}{\sigma_0^2}\\
&= \frac{(t - \mu_0)^2}{\sigma_T^2 + \sigma_0^2}
\end{align}

Then:
\begin{align}
p(\theta | t)
&\propto e^{-\frac{1}{2}\frac{(t - \mu_0)^2}{\sigma_T^2 + \sigma_0^2} - \frac{1}{2}\frac{(\mu - \mu_1)^2}{\sigma_1^2}}\delta(\sigma - \sigma_T)\\
&\propto e^{- \frac{1}{2}\frac{(\mu - \mu_1)^2}{\sigma_1^2}}\delta(\sigma - \sigma_T)\\
&=\mathcal{N}(\mu; \mu_1, \sigma_1^2) \delta(\sigma - \sigma_T)
\end{align}

where
\begin{align}
\left\{
\begin{array}{l}
\mu_1=\frac{\frac{t}{\sigma_T^2} + \frac{\mu_0}{\sigma_0^2}}{\frac{1}{\sigma_T^2} + \frac{1}{\sigma_0^2}} \\ [1em]
\sigma_1^2=\frac{1}{\frac{1}{\sigma_T^2} + \frac{1}{\sigma_0^2}}
\end{array}
\right.
\end{align}
\end{example}

Bayesian inference is online, integrates epistemic uncertainty and mixes prior beliefs with observations.

\subsection{The problem of choosing a prior}

The choice of a prior distribution $p(\theta)$ is a fundamental aspect of Bayesian inference. Philosophically, the prior represents existing knowledge before observing the data. When little prior knowledge is available, a more dispersed or non-informative prior is used, whereas a well-chosen prior can significantly impact the speed and quality of convergence.

A crucial aspect of priors is their connection to regularization, particularly in scenarios with limited data. The necessity of a prior often leads to criticism of Bayesian methods for introducing subjectivity. However, priors also provide flexibility and structure in probabilistic modeling, distinguishing Bayesian approaches from frequentist methods. The discussion between Bayesian and frequentist perspectives will be explored further in the course.

In practical applications, prior selection involves a trade-off. One option is to choose a tractable but potentially unrealistic prior, which enables exact inference. This is the case with conjugate priors, which simplify computations by ensuring that the posterior remains within the same family of distributions as the prior. This property is particularly useful in Bayesian updating, as it allows for analytical solutions and computational efficiency. However it does not fit reality.

Another approach is to use a more representative but intractable prior, requiring approximate inference techniques such as Markov Chain Monte Carlo (MCMC) sampling or variational inference.

% In Section 3.5, we discuss uninformative priors, which often correspond to a limit of a conjugate prior where we ``know nothing.''
% In Section 3.6, we discuss hierarchical priors, which are useful when we have multiple related datasets.
% In Section 3.7, we discuss empirical priors, which can be learned from the data.

\subsection{Maximum A Posteriori (MAP)}
The maximum a posteriori (MAP) estimation is a mode of the posterior distribution. It provides a point estimate of the parameter $\theta$ that maximizes the posterior probability given the observed data and prior knowledge. Formally, the MAP estimate is defined as:

\begin{equation}
\hat{\theta}_{map} = \arg\max\limits_{\theta} p(\theta|\mathcal{D},\kappa)
\end{equation}

where $\mathcal{D}$ represents the observed data and $k$ denotes additional prior knowledge or hyperparameters.

MAP estimation is one of the most widely used methods in Bayesian inference because it transforms the posterior maximization into an optimization problem. Furthermore, MAP estimation can be interpreted from a non-Bayesian perspective. By expressing the log-prior distribution as a regularization term, MAP estimation resembles regularized maximum likelihood estimation, which is frequently employed in machine learning models. However, despite its popularity and computational efficiency, MAP estimation presents several important drawbacks.

\textbf{No measure of uncertainty:} Like other point estimates (e.g., posterior mean or median), the MAP estimate does not convey any information about the uncertainty of the parameter. Since the posterior distribution is summarized by a single point, the intrinsic uncertainty of the distribution is lost.

\textbf{Overconfidence in predictions:} Plugging the MAP estimate directly into a predictive model can result in overconfident predictions. This is particularly problematic in risk-averse situations, where underestimating uncertainty can lead to overly optimistic decisions.

\textbf{Mode as an untypical point:} The MAP estimate corresponds to the mode of the posterior distribution, which can be an untypical point in many cases. Unlike the posterior mean or median, the mode does not account for the volume of the parameter space. This issue is especially pronounced in skewed distributions. For instance, in such cases, the mode may lie far from the bulk of the probability mass, making it a poor summary of the posterior distribution.

These limitations highlight the importance of complementing MAP estimation with more informative summaries of the posterior distribution, such as credible intervals or full posterior samples, whenever possible.

\begin{example}{MAP for network example \ref{ex:network_problem}}
We showed that:

\begin{align}
p(\theta | \mathcal{D}) =\mathcal{N}(\mu; \frac{\frac{n}{\sigma_T^2} \bar{t}+ \frac{\mu_0}{\sigma_0^2}}{\frac{n}{\sigma_T^2} + \frac{1}{\sigma_0^2}},\frac{1}{\frac{n}{\sigma_T^2} + \frac{1}{\sigma_0^2}})
\end{align}
Note here we have the likelihood on whole dataset and $t$ is replace by mean $\bar{t}=\frac{1}{n}\sum_{i=1}^n t_i$ and the apparition of n in the parameters formula. This leads to:
$$\hat{\mu}_{MAP}=\frac{\frac{n}{\sigma_T^2} \bar{t}+ \frac{\mu_0}{\sigma_0^2}}{\frac{n}{\sigma_T^2} + \frac{1}{\sigma_0^2}}$$

\end{example}

\subsection{Key differences: MLE vs MAP}
While MLE focuses solely on observed data, MAP estimation incorporates prior information. MAP estimates parameters as:
\begin{equation}
\hat{\theta}_{\text{map}} = \arg\max_{\theta} p(\theta | \mathcal{D}) = \arg\max_{\theta} \log p(\theta | \mathcal{D}) = \arg\max_{\theta} \left[ \log p(\theta) + \log p(\mathcal{D} | \theta) \right]
\end{equation}

where $p(\theta)$ is the prior. This makes MAP a regularized form of MLE.

\subsection{Maximum A Posteriori Estimator (MAP) revisited}
MLE is equivalent to MAP with uniform (possibly improper) prior. We have non dependence on prior parametrization as MAP but the variance is higher compared to MAP.
\begin{equation*}
\hat{\theta}_{map} = \arg\max\limits_{\theta} p(\theta | \mathcal{D}) 
= \arg\max\limits_{\theta} \left( p(\theta) p(\mathcal{D} | \theta) \right) 
= \arg\max\limits_{\theta} p(\mathcal{D} | \theta) 
= \hat{\theta}_{mle}
\end{equation*}


\subsection{Bayes estimator}
As Bayesian inference often has no simple analytic form, or even, is hardly computable and some problems require to choose values for $\theta$ for real-time prediction, \textbf{Bayes estimator} is a solution. In Bayesian decision theory, we treat the data as an observed constant, $\mathcal{D}$, and the state of nature as an unknown random variable. The \textbf{posterior expected loss} or \textbf{Bayesian risk} for picking action is defined as follows:

\begin{equation}
\rho(\theta^{'} | \mathcal{D}) \triangleq \mathbb{E}_{p(\theta | \mathcal{D})} \left[ \ell(\theta, \theta^{'}) \right] = \int \ell(\theta, \theta^{'}) p(\theta | \mathcal{D}) \, d \theta
\end{equation}

where $\ell(\theta,\theta^{'})$ is the cost of choosing $\theta^{'}$ when real value is $\theta$. The optimal policy specifies what action to take so as to minimize the expected loss. Hence the \textbf{Bayes estimator}, also called the \textbf{Bayes decision rule}, is given by:

\begin{equation}
    \hat{\theta}_{bayes}= \arg\min\limits_{\theta^{'}}\rho(\theta^{'} | \mathcal{D})
\end{equation}

We remark that this is not Bayesian anymore, strictly speaking. Plus, epistemic uncertainty is lost.

\begin{example}{MAP is the Bayes estimator for a \textbf{uniform loss} $l(\theta,\theta^{'})=1-\delta(\theta,\theta^{'})$}
\begin{align}
\hat{\theta}_{bayes}
&=\arg\min\limits_{\theta^{'}}\rho(\theta^{'}|\mathcal{D})\\
&=\arg\min\limits_{\theta^{'}}\mathbb{E}_{p(\theta | \mathcal{D})} \left[ \ell(\theta, \theta^{'}) \right]\\
&=\arg\min\limits_{\theta^{'}}\mathbb{E} \left[1-\delta(\theta, \theta^{'}) \right]\\
&=\arg\max\limits_{\theta^{'}}\mathbb{E} \left[\delta(\theta, \theta^{'}) \right]\\
&=\arg\max\limits_{\theta^{'}}\int \delta(\theta, \theta^{'}) p(\theta | \mathcal{D}) \, d \theta\\
&=\arg\max\limits_{\theta^{'}}p(\theta | \mathcal{D})\\
&=\hat{\theta}_{MAP}
\end{align}
\end{example}

\begin{example}[MLE of a Bernoulli variable]
Given $n$ i.i.d samples $\mathcal{D} = (x_i)_{1 \leq i \leq n}$ of $X \sim Ber(p)$, we derive the expression of MLE $\hat{p}_{mle}$:

\begin{align}
\log \mathcal{L}(\mathcal{D};p) &= \sum_{i=1}^{n} \log \mathcal{L}(x_i; p) = \sum_{i=1}^{n} \log (\mathbb{P}(x_i | p)\\
&= \sum_{x_i=1} \log \mathbb{P}(1| p) + \sum_{x_i=0} \log \mathbb{P}(0| p)\\
&= n_1 \log(p) + n_0 \log(1 - p)
\end{align}

\begin{align}
\frac{\partial}{\partial p} \log \mathcal{L}(\mathcal{D};\hat{p}_{mle}) = 0
&\Leftrightarrow \frac{n_1}{\hat{p}_{mle}} + \frac{n - n_1}{\hat{p}_{mle} - 1} = 0 \\
&\Leftrightarrow \hat{p}_{mle} = \frac{n_1}{n}
\end{align}

where $n_0=\sum_{i=1}^{n} \mathbbm{1}_{x_i=0}$, $n_1=\sum_{i=1}^{n} \mathbbm{1}_{x_i=1}$ and $n=n_0+n_1$.\\

The result generalizes to non-binary categorical variables $X \sim Cat(p)$ with $p = (p_1, ..., p_k)$:

\begin{align}
&\nabla \left( \log \mathcal{L}(\mathcal{D};p) - \lambda \left( \sum_{v=1}^{k} p_v - 1 \right) \right)(\hat{p}_{mle}) = 0\\
&\Leftrightarrow \frac{\partial}{\partial p_j} \left( \sum_{v=1}^{k} (n_v \log(p_v) - \lambda p_v) \right) = 0 \\
&\Leftrightarrow \hat{p}_v^{(mle)} = \frac{n_v}{n}
\end{align}

where $n_v=\sum_{i=1}^{n} \mathbbm{1}_{x_i=v}$ and $n=\sum_{v=1}^{k} n_v$.\\

MLE for a categorical distribution amounts to compute frequency of occurrences in data.
\end{example}

\subsection{Beta distribution}
The beta distribution has support over the interval $[0,1]$ and is defined as follows:

\begin{equation}
\text{Beta}(x|a, b) = \frac{x^{a-1} (1 - x)^{b-1}}{B(a, b)}
\end{equation}

Here, $B(a, b)$ is the beta function,

\begin{equation}
B(a, b) = \frac{\Gamma(a) \Gamma(b)}{\Gamma(a + b)}
\end{equation}

\begin{align}
X \sim Beta(a,b) \Rightarrow
\left\{
\begin{array}{l}
\mathbb{E}(X)=\frac{a}{a+b} \\ [1em]
\text{Var(X)}=\frac{ab}{(a+b)^2(a+b+1)} \\ [1em]
\text{Mode(X)}=\frac{a-1}{a+b-2}
\end{array}
\right.
\end{align}

The Beta distribution is a conjugate prior or Bernoulli distributions. Let $p(\theta)=\text{Beta}(a,b)$ be the prior, $\mathcal{D}=\{x_i\}_{1 \leq i \leq n}$ with $x_i \sim \text{Ber}(\textbf{p})$. Thus, the posterior is:

% TODO: Page 17 des slide du cours estimation and bayesian inference.
% \begin{align}
% \end{align}

\subsection{Pareto distribution}
The Pareto distribution is used to model the distribution of quantities that exhibit long tails, also called heavy tails. For example, it has been observed that the most frequent word in English (“the”) occurs approximately twice as often as the second most frequent word (“of”), which occurs twice as often as the fourth most frequent word, etc. If we plot the frequency of words vs their rank, we will get a power law; this is known as Zipf’s law. Wealth has a similarly skewed distribution, especially in plutocracies such as the USA.

The Pareto pdf is defined as follows:
\begin{equation}
    \text{Pareto}(x | k, m) = \frac{k m^k}{x^{k+1}} \mathbbm{1}_{x \geq m}
\end{equation}
This density asserts that $x$ must be greater than some constant $m$, but not too much greater, where $k$ controls what is “too much”. As $k \to \infty$, the distribution approaches $\delta(x - m)$.

% TODO: Add plot: see page 44 of ML a proba perspective
If we plot the distribution on a log-log scale, it forms a straight line, of the form
\[
    \log p(x) = a \log x + c
\]
for some constants $a$ and $c$.

\begin{align}
X \sim Pareto(k,m) \Rightarrow
\left\{
\begin{array}{l}
    \mathbb{E}(X)=\frac{km}{k-1}, if k>m \\ [1em]
\text{Var(X)}=m \\ [1em]
    \text{Mode(X)}=\frac{m^2 k}{(k-1)^2(k-2)}
\end{array}
\right.
\end{align}

\subsection{Dirichlet distribution}
A multivariate generalization of the Beta distribution is the Dirichlet distribution, which has support over the probability simplex, defined by:
$$S_K = \left\{ x : 0 \leq x_k \leq 1, \quad \sum_{k=1}^{K} x_k = 1 \right\}$$

where $x_k$ represents the $k$-th component of the vector $x$, and the sum of all components is constrained to be equal to 1. The Dirichlet distribution is defined as follows:
\begin{equation}
    \text{Dir}(x|\boldsymbol{\alpha}) = \frac{1}{B(\boldsymbol{\alpha})} \prod_{k=1}^{K} x_k^{\alpha_k - 1} \mathbbm{1}_{x \in S_K}
\end{equation}
where $B(\boldsymbol{\alpha})$ is the normalization constant, defined as the natural generalization of the beta function to $K$ variables:

$$B(\boldsymbol{\alpha}) =\frac{\prod_{k=1}^{K}\Gamma(\alpha_k)}{\Gamma(\alpha_0)}$$

and $\alpha_0 = \sum_{k=1}^{K} \alpha_k$

\begin{align}
\boldsymbol{X} \sim \text{Dir}(\boldsymbol{\alpha}) \Rightarrow
\left\{
\begin{array}{l}
\mathbb{E}(x_k)=\frac{\alpha_k}{\alpha_0} \\ [1em]
\text{Var}(x_k)=\frac{\alpha_{k}-1}{\alpha_{0} - K} \\ [1em]
\text{Mode}(x_k)=\frac{\alpha_k(\alpha_0 - \alpha_k)}{\alpha_0^2(\alpha_0 + 1)}
\end{array}
\right.
\end{align}

% TODO: Co,njugate prior of dirichlet
% TODO: MAP of categorical distribution: see ML proba perspective book

\subsection{The log-sum-exp trick}
When working with generative classifiers or models involving probability distributions, one common challenge is numerical stability, particularly when dealing with very small probabilities. This can lead to issues like underflow or overflow, especially in high-dimensional spaces, where the likelihood function might involve the product of many small probabilities.

To mitigate this, it is often beneficial to use the log of the likelihood function. Taking the logarithm of probabilities helps transform very small numbers into more manageable ones, making the computation numerically stable. This can be achieved by computing the log-likelihood:

\begin{equation}
\log \mathcal{L}(\mathcal{D};\theta) = \sum_{i=1}^{n} \log \mathcal{L}(x_i;\theta)
\end{equation}

This log transformation converts the product of probabilities into a sum of logs, which is much less prone to numerical instability. The log-likelihood is often maximized in practice, as we are typically interested in finding the parameter values $\hat{\theta}_{mle}$ that maximize this quantity:

\begin{equation}
\hat{\theta}_{mle} = \arg \max_{\theta} \log \mathcal{L}(\mathcal{D};\theta)
\end{equation}

This approach effectively addresses the issue of underflow by ensuring that the likelihood computation stays within a stable numerical range.\\

While transforming the likelihood into a log-likelihood helps prevent underflow, there are still other potential numerical issues, such as overflow, which can occur when working with large numbers. In situations where the arguments of an exponential function are extremely large (either very positive or very negative), we may encounter overflow or underflow errors. For instance, consider the softmax function, which is commonly used in classification tasks to compute probabilities from logits:

\begin{equation}
\hat{p}_i = \frac{\exp(z_i)}{\sum_{j=1}^{n} \exp(z_j)}
\end{equation}

Here, the numerator and denominator involve exponentiating the values of $z_i$. If the values of $z_i$ are too large or too small, the exponential function can produce values that exceed the limits of the floating-point representation, leading to overflow or underflow. For example, if $z_i$ is very large, the exponential term can become larger than the largest number representable in the system, causing overflow. Conversely, if $z_i$ is very negative, the result of the exponential term can become so small that it results in underflow.

To deal with this, we can apply a technique known as the \textbf{max trick}, which involves subtracting the maximum value from all the entries before computing the exponentials:

\begin{equation}
\hat{p}_i = \frac{\exp(z_i - \max_j z_j)}{\sum_{j=1}^{n} \exp(z_j - \max_k z_k)}
\end{equation}

This trick ensures that the largest exponent is zero, thus preventing overflow, while the differences between all the terms remain the same. This transformation keeps the values within a numerically stable range. Additionally, this ensures that the exponentials remain well-behaved and the probabilities do not exceed 1 or become numerically unstable.

In many machine learning algorithms, especially when computing sums over exponentials of many terms, such as in Bayesian inference or softmax computations, the log-sum-exp trick is widely used. The key idea is to avoid directly computing sums in the exponential domain, which can lead to significant precision errors when adding up small numbers. The \textbf{log-sum-exp} trick involves factoring out the largest term in the sum and representing the remaining terms relative to it.

For a set of probabilities $\{p_i\}_{1 \leq i \leq n}$, we can use both of the methods:

\begin{align}
p_i &\rightarrow \log(p_i) \quad \text{(log trick)} \\
&\rightarrow \log(p_i) - \max_i(\log(p_i) \quad \text{(max trick)} \\
&\rightarrow \frac{e^{\log(p_i) - \max_i(\log(p_i)}}{\sum_{i=1}^n e^{\log(p_i) - \max_i(\log(p_i)}} \quad \text{(exp and sum trick)}
\end{align}




%%%%%%%%%% Appendix
% \newpage
% \input{./src/appendix.tex}

\newpage
\section{References}
\begin{itemize}
    \item Murphy, K. P. (2012). \textit{Machine Learning: a Probabilistic Perspective}. MIT Press. \url{https://probml.github.io/pml-book/book0.html}
    \item Murphy, K. P. (2023). \textit{Probabilistic Machine Learning: Advanced Topics}. MIT Press. \url{http://probml.github.io/book2}
    \item Statistical Models - SDI Metz
\end{itemize}

\end{document}
