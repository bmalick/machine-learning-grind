
# machine learning and deep learning roadmap learning

- Learning Machine Learning and Deep Learning based on books [Dive into Deep Learning] & [Hands-on Machine Learning with Scikit-Learn Keras & TensorFlow]

- Using PyTorch

- Inspired by saurabhaloneai from github



## Totalcount : (0/60)
<!-- ✅ -->
<!-- 
| Task           | Status     |
| -------------- | ---------- |
|                |            | -->

## 01-machine-learning
<!-- ✅ -->

| Task                        | Status     |
| --------------------------- | ---------- |
| 01-stats                    |            |
| 02-linear-regression        |            |
| 03-svm                      |            |
| 04-decision-tree            |            |
| 05-ensemble-learning        |            |
| 06-dimensionality-reduction |            |
| 07-unsupervised-learning    |            |

## 02-deep-neural-networks
<!-- ✅ -->

| Task              | Status     |
| ----------------- | ---------- |
| 01-le-net         |            |
| 02-alex-net       |            |
| 03-google-le-net  |            |
| 04-vgg-net        |            |
| 05-res-net        |            |
| 06-xception       |            |
| 07-se-net         |            |
| 08-unet           |            |

## 03-optimization-and-regularization
<!-- ✅ -->

| Task                       | Status     |
| -------------------------- | ---------- |
| 00-backpropagation         |            |
| 01-initializations         |            |
| 02-activations             |            |
| 03-losses                  |            |
| 04-optimizers              |            |
| 05-learning-rate-schedule  |            |
| 06-early-stopping          |            |
| 07-batch-norm              |            |
| 08-layer-norm              |            |
| 09-gradient-clipping       |            |
| 10-weights-decay           |            |
| 11-dropout                 |            |

## 04-cnn
<!-- ✅ -->
| Task                    | Status     |
| ----------------------- | ---------- |
| 01-convolutions-filters |            |
| 02-padding-and-stride   |            |
| 03-channels             |            |
| 04-pooling              |            |

## 05-rnn
<!-- ✅ -->
| Task                            | Status     |
| ------------------------------- | ---------- |
| 01-rnn                          |            |
| 02-backpropagation-through-time |            |
| 03-lstm                         |            |
| 04-gru                          |            |
| 05-deep-rnn                     |            |
| 06-bidirectional-rnn            |            |
| 07-machine-learning-translation |            |
| 08-encoder-decoder-architecture |            |
| 09-seq2seq                      |            |
| 10-beam-search                  |            |

## 06-transformers
<!-- ✅ -->
| Task                        | Status     |
| --------------------------- | ---------- |
| 01-attention                |            |
| 02-multihead-attention      |            |
| 03-positional-encoding      |            |
| 04-transformer-architecture |            |
| 05-transformer-for-vision   |            |


## 07-gans
<!-- ✅ -->

## 08-reinforcement-learning
<!-- ✅ -->

## 09-hyperparameter-optimization
<!-- ✅ -->

## 10-recommender-systems
<!-- ✅ -->

# Papers

<!-- Machine learning -->

<!-- Deep neural network -->
- [ ] **DNN** - Learning Internal Representations by Error Propagation [[pdf]](https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap8_PDP86.pdf)
- [ ] **Train DNN** - Training Very Deep Networks (2015) [[pdf]](https://proceedings.neurips.cc/paper_files/paper/2015/file/215a71a12769b056c3c32e7299f1c5ed-Paper.pdf)
- [ ] **Deep Learning** - Deep Learning (2015) [[pdf]](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf)
- [ ] **Knowledge Distillation** - Distilling the Knowledge in a Neural Network (2015) [[pdf]](https://arxiv.org/pdf/1503.02531)
- [ ] **Transferable Features** - How transferable are features in deep neural networks? (2014) [[pdf]](https://proceedings.neurips.cc/paper_files/paper/2014/file/375c71349b295fbe2dcdca9206f20a06-Paper.pdf)
- [ ] **RNN** - A Learning Algorithm for Continually Running Fully Recurrent Neural Networks (1989), R. J. Williams [[pdf]](https://gwern.net/doc/ai/nn/rnn/1989-williams-2.pdf)
- [ ] **LSTM** - Long-Short Term Memory (1997), S. Hochreiter and J. Schmidhuber [[pdf]](https://www.bioinf.jku.at/publications/older/2604.pdf)
- [ ] **Learning to Forget** - Learning to Forget: Continual Prediction with LSTM (2000), F. A. Gers et al. [[pdf]](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e10f98b86797ebf6c8caea6f54cacbc5a50e8b34)

<!-- Optimization and regularization -->
- [ ] **Xavier Initialization** - Understanding the difficulty of training deep feedforward neural networks [[pdf]](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
- [ ] **Adam** - Adam: A Method for Stochastic Optimization (2014) [[pdf]](https://arxiv.org/pdf/1412.6980)
- [ ] **BatchNorm** - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (2015) [[pdf]](https://arxiv.org/pdf/1502.03167)
- [ ] **Dropout** - Dropout: A Simple Way to Prevent Neural Networks from Overfitting (2014) [[pdf]](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)
- [ ] **LayerNorm** - Layer Normalization (2016) [[pdf]](https://arxiv.org/pdf/1607.06450v1)
- [ ] **Weight Decay** - A Simple Weight Decay Can Improve Generalization (1991), A. Krogh and J. Hertz [[pdf]](https://proceedings.neurips.cc/paper/1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf)
- [ ] **ReLU** - Deep Sparse Rectified Neural Networks (2011), X. Glorot et al. [[pdf]](https://www.researchgate.net/publication/215616967_Deep_Sparse_Rectifier_Neural_Networks)
- [ ] **GELU** - Gaussian Error Linear Units (GELUs) (2016), D. Hendrycks and K. Gimpel [[pdf]](https://arxiv.org/pdf/1606.08415)
- [ ] **PPO** - Proximal Policy Optimization Algorithms (2017) [[pdf]](https://arxiv.org/pdf/1707.06347)

<!-- Computer vision -->
- [ ] **LeNet** - Gradient-Based Learning Applied to Document Recognition [[pdf]](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)
- [ ] **CNN** - Backpropagation Applied to Handwritten Zip Code Recognition [[pdf]](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf)
- [ ] **Deep CNN** - Very Deep Convolutional Networks for Large-Scale Image Recognition (2015) [[pdf]](https://arxiv.org/pdf/1409.1556)
- [ ] **Deep Convolution** - Going Deeper with Convolutions (2014) [[pdf]](https://arxiv.org/pdf/1409.4842)
- [ ] **AlexNet** - ImageNet Classification with Deep Convolutional Neural Network [[pdf]](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
- [ ] **ResNet** - Deep Residual Learning for Image Recognition [[pdf]](https://arxiv.org/pdf/1512.03385)
- [ ] **Transfer Learning CNN** - Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks (2014) [[pdf]](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf)
- [ ] **DeepFace** - DeepFace: Closing the Gap to Human-Level Performance in Face Verification (2014) [[pdf]]https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf()
- [ ] **Mask R-CNN** - Mask R-CNN (2018) [[pdf]](https://arxiv.org/pdf/1703.06870)
- [ ] **Understand CNN** - Visualizing and Understanding Convolutional Networks (2013) [[pdf]](https://arxiv.org/pdf/1311.2901)
- [ ] **Inception** - Rethinking the Inception Architecture for Computer Vision (2015) [[pdf]](https://arxiv.org/pdf/1512.00567)
- [ ] **Inception-v4** - Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning (2016) [[pdf]](https://arxiv.org/pdf/1602.07261)
- [ ] **Mappings in ResNet** - Identity Mappings in Deep Residual Networks (2016) [[pdf]](https://arxiv.org/pdf/1603.05027v2)
- [ ] **Spatial Transformer** - Spatial Transformer Networks (2015) [[pdf]](https://arxiv.org/abs/1506.02025)
- [ ] **Network in Network** - Network In Network (2014) [[pdf]](https://arxiv.org/pdf/1312.4400)
- [ ] **YOLO** - You Only Look Once: Unified, Real-Time Object Detection (2016) [[pdf]](https://arxiv.org/pdf/1506.02640)
- [ ] **Image Segmentation** - Fully Convolutional Networks for Semantic Segmentation (2015) [[pdf]](https://arxiv.org/pdf/1411.4038)
- [ ] **U-net** - U-Net: Convolutional Networks for Biomedical Image Segmentation (2015) [[pdf]](https://arxiv.org/abs/1505.04597)
- [ ] **Fast R-CNN** - Fast R-CNN (2015) [[pdf]](https://arxiv.org/pdf/1504.08083)
- [ ] **Faster R-CNN** - Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (2015) [[pdf]](https://arxiv.org/pdf/1506.01497)
- [ ] **CNN for Video** - Image Super-Resolution Using Deep Convolutional Networks (2016) [[pdf]](https://arxiv.org/pdf/1501.00092)
- [ ] **Image Captioning** - Show, Attend and Tell: Neural Image Caption Generation with Visual Attention (2015) [[pdf]](https://arxiv.org/pdf/1502.03044)
- [ ] **Long-term R-CNN** - Long-term Recurrent Convolutional Networks for Visual Recognition and Description (2016) [[pdf]](https://arxiv.org/pdf/1411.4389)
- [ ] **VQA** - VQA: Visual Question Answering (2015) [[pdf]](https://arxiv.org/pdf/1505.00468)
- [ ] **Two-Steram CNN** - Two-Stream Convolutional Networks for Action Recognition in Video (2014) [[pdf]](https://arxiv.org/pdf/1406.2199)
- [ ] **GAN** - Generative Adversarial Networks (2014) [[pdf]](https://arxiv.org/pdf/1406.2661)
- [ ] **GAN** - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (2016) [[pdf]](https://arxiv.org/pdf/1511.06434)
- [ ] **DCGAN** - () [[pdf]]()
- [ ] **Improve GANs** - Improved Techniques for Training GANs (2016) [[pdf]](https://proceedings.neurips.cc/paper_files/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf)
- [ ] **Image2StyleGAN** - Image2StyleGAN: How to Embed Images Into the StyleGAN Latent Space? (2019) [[pdf]](https://arxiv.org/pdf/1904.03189)
- [ ] **Vision Transformer** - An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020) [[pdf]](https://arxiv.org/pdf/2010.11929)
- [ ] **Diffusion** - Deep Unsupervised Learning using Nonequilibrium Thermodynamics (2015) [[pdf]](https://arxiv.org/pdf/1503.03585)
- [ ] **DALLE** - Zero-Shot Text-to-Image Generation (2021) [[pdf]](https://arxiv.org/pdf/2102.12092)
- [ ] **DALLE 2** - Hierarchical Text-Conditional Image Generation with CLIP Latents (2022) [[pdf]](https://arxiv.org/pdf/2204.06125)
- [ ] **CLIP** - Learning Transferable Visual Models From Natural Language Supervision (2021) [[pdf]](https://arxiv.org/pdf/2103.00020)

<!-- NLP & Transformers -->
- [ ] **Encoder-Decoder** - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation (2014) [[pdf]](https://arxiv.org/pdf/1406.1078)
- [ ] **Seq2Seq** - Sequence to Sequence Learning with Neural Networks [[pdf]](https://arxiv.org/pdf/1409.3215)
- [ ] **GloVe** - GloVe: Global Vectors for Word Representation (2014) [[pdf]](https://nlp.stanford.edu/pubs/glove.pdf)
- [ ] **Attention Speech Recognition** - End-to-End Attention-based Large Vocabulary Speech Recognition (2016) [[pdf]](https://arxiv.org/pdf/1508.04395)
- [ ] **Attention** - Neural Machine Translation by Jointly Learning to Align and Translate (2016) [[pdf]](https://arxiv.org/pdf/1409.0473)
- [ ] **Transformer** - Attention Is All You Need (2017) [[pdf]](https://arxiv.org/pdf/1706.03762)
- [ ] **Deep Speech** - Deep Speech 2: End-to-End Speech Recognition in English and Mandarin (2015) [[pdf]](https://arxiv.org/pdf/1512.02595)
- [ ] **Speech Recognition** - Speech Recognition with Deep Recurrent Neural Networks (2013) [[pdf]](https://arxiv.org/pdf/1303.5778)
- [ ] **BLEU** - BLEU: A Method for Automatic Evaluation of Machine Translation [[pdf]](https://aclanthology.org/P02-1040.pdf)
- [ ] **ROUGE** - ROUGE: A Package for Automatic Evaluation of Summaries [[pdf]](https://aclanthology.org/W04-1013.pdf)
- [ ] **SuperGlue** - SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems [[pdf]](https://w4ngatang.github.io/static/papers/superglue.pdf)
- [ ] **BLOOM** - BLOOM: A 176B-Parameter Open-Access Multilingual Language Model [[pdf]](https://arxiv.org/pdf/2211.05100)
- [ ] **PEFT** - Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning [[pdf]](https://arxiv.org/pdf/2303.15647)
- [ ] **PEFT Effectiveness** - On the Effectiveness of Parameter-Efficient Fine-Tuning [[pdf]](https://arxiv.org/pdf/2211.15583)
- [ ] **LoRA** LoRA: Low-Rank Adaptation of Large Language Models [[pdf]](https://arxiv.org/pdf/2106.09685)
- [ ] **QLoRa** - QLoRA: Efficient Finetuning of Quantized LLMs [[pdf]](https://arxiv.org/pdf/2305.14314)
- [ ] **GPT** - Improving Language Understanding by Generative Pre-Training (2018) [[pdf]](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [ ] **GPT-2** - Language Models are Unsupervised Multitask Learners (2018) [[pdf]](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [ ] **GPT-3** - Language Models are Few-Shot Learners (2020) [[pdf]](https://arxiv.org/pdf/2005.14165)
- [ ] **GPT-4** - GPT-4 Technical Report (2023), OpenAI [[pdf]](https://arxiv.org/pdf/2303.08774)
- [ ] **Word2Vec** - Efficient Estimation of Word Representations in Vector Space (2013) [[pdf]](https://arxiv.org/pdf/1301.3781)
- [ ] **Phrase2Vec** - Distributed Representations of Words and Phrases and their Compositionality (2013) [[pdf]](https://arxiv.org/pdf/1310.4546)
- [ ] **Mixture of Experts** - Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (2017) [[pdf]](https://arxiv.org/pdf/1701.06538)
- [ ] **BERT** - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018) [[pdf]](https://arxiv.org/pdf/1810.04805)
- [ ] **RoBERTa** - RoBERTa: A Robustly Optimized BERT Pretraining Approach (2019) [[pdf]](https://arxiv.org/pdf/1907.11692)
- [ ] **T5** - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (2019) [[pdf]](https://arxiv.org/pdf/1910.10683)
- [ ] **Prompt Tuning** - The Power of Scale for Parameter-Efficient Prompt Tuning (2021) [[pdf]](https://arxiv.org/pdf/2104.08691)
- [ ] **InstructGPT** - Training language models to follow instructions with human feedback (2022) [[pdf]](https://arxiv.org/pdf/2203.02155)

<!-- Time Series -->
- [ ] **WaveNet** - WaveNet: A Generative Model for Raw Audio (2016) [[pdf]](https://arxiv.org/pdf/1609.03499)
- [ ] **Whisper** - Robust Speech Recognition via Large-Scale Weak Supervision (2022) [[pdf]](https://arxiv.org/pdf/2212.04356)
- [ ] **MMS** - Scaling Speech Technology to 1,000+ Languages (2023) [[pdf]](https://arxiv.org/pdf/2305.13516)

<!-- Reinforcement learning -->
- [ ] **Reinforcement Learning** - Playing Atari with Deep Reinforcement Learning (2013) [[pdf]](https://arxiv.org/pdf/1312.5602)
- [ ] **RLHF** - Fine-Tuning Language Models From Human Preferences (2019), D. Ziegler et al. [[pdf]](https://arxiv.org/pdf/1909.08593)
- [ ] **RLHF** - Training language models to follow instructions with human feedback (2022), L. Ouyang, J. Wu et al. [[pdf]](https://arxiv.org/pdf/2203.02155)
- [ ] **RLHF** - Learning to summarize from human feedback (2022), N. Stiennon, L. Ouyang, J. Wu, et al. [[pdf]](https://arxiv.org/pdf/2009.01325)



<!-- - [ ] **** -  () [[pdf]]() -->
<!-- - [ ] **** -  () [[pdf]]() -->

