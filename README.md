
# machine learning and deep learning roadmap learning

- Learning Machine Learning and Deep Learning based on books [Dive into Deep Learning] & [Hands-on Machine Learning with Scikit-Learn Keras & TensorFlow]

- Using PyTorch

- Inspired by saurabhaloneai from github



## Totalcount : (0/60)
<!-- ✅ -->
<!-- 
| Task           | Status     |
| -------------- | ---------- |
|                |            | -->

## 01-machine-learning
<!-- ✅ -->

| Task                        | Status     |
| --------------------------- | ---------- |
| 01-stats                    |            |
| 02-linear-regression        |            |
| 03-svm                      |            |
| 04-decision-tree            |            |
| 05-ensemble-learning        |            |
| 06-dimensionality-reduction |            |
| 07-unsupervised-learning    |            |

## 02-deep-neural-networks
<!-- ✅ -->

| Task              | Status     |
| ----------------- | ---------- |
| 01-le-net         |            |
| 02-alex-net       |            |
| 03-google-le-net  |            |
| 04-vgg-net        |            |
| 05-res-net        |            |
| 06-xception       |            |
| 07-se-net         |            |
| 08-unet           |            |

## 03-optimization-and-regularization
<!-- ✅ -->

| Task                       | Status     |
| -------------------------- | ---------- |
| 00-backpropagation         |            |
| 01-initializations         |            |
| 02-activations             |            |
| 03-losses                  |            |
| 04-optimizers              |            |
| 05-learning-rate-schedule  |            |
| 06-early-stopping          |            |
| 07-batch-norm              |            |
| 08-layer-norm              |            |
| 09-gradient-clipping       |            |
| 10-weights-decay           |            |
| 11-dropout                 |            |

## 04-cnn
<!-- ✅ -->
| Task                    | Status     |
| ----------------------- | ---------- |
| 01-convolutions-filters |            |
| 02-padding-and-stride   |            |
| 03-channels             |            |
| 04-pooling              |            |

## 05-rnn
<!-- ✅ -->
| Task                            | Status     |
| ------------------------------- | ---------- |
| 01-rnn                          |            |
| 02-backpropagation-through-time |            |
| 03-lstm                         |            |
| 04-gru                          |            |
| 05-deep-rnn                     |            |
| 06-bidirectional-rnn            |            |
| 07-machine-learning-translation |            |
| 08-encoder-decoder-architecture |            |
| 09-seq2seq                      |            |
| 10-beam-search                  |            |

## 06-transformers
<!-- ✅ -->
| Task                        | Status     |
| --------------------------- | ---------- |
| 01-attention                |            |
| 02-multihead-attention      |            |
| 03-positional-encoding      |            |
| 04-transformer-architecture |            |
| 05-transformer-for-vision   |            |


## 07-gans
<!-- ✅ -->

## 08-reinforcement-learning
<!-- ✅ -->

## 09-hyperparameter-optimization
<!-- ✅ -->

## 10-recommender-systems
<!-- ✅ -->

# Papers

- [ ] **Xavier Initialization** - Understanding the difficulty of training deep feedforward neural networks [[PDF]](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)

- [ ] **DNN** - Learning Internal Representations by Error Propagation (1987), D. E. Rumelhart et al. [[PDF]](https://stanford.edu/~jlmcc/papers/PDP/Volume%201/Chap8_PDP86.pdf)
- [ ] **CNN** - Backpropagation Applied to Handwritten Zip Code Recognition (1989), Y. Lecun et al. [[PDF]](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf)
- [ ] **LeNet** - Gradient-Based Learning Applied to Document Recognition (1998), Y. Lecun et al. [[PDF]](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)
- [ ] **AlexNet** - ImageNet Classification with Deep Convolutional Networks (2012), A. Krizhevsky et al. [[PDF]](https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
- [ ] **U-Net** - U-Net: Convolutional Networks for Biomedical Image Segmentation (2015), O. Ronneberger et al. [[PDF]](https://arxiv.org/pdf/1505.04597)
- [ ] **Weight Decay** - A Simple Weight Decay Can Improve Generalization (1991), A. Krogh and J. Hertz [[PDF]](https://proceedings.neurips.cc/paper/1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf)
- [ ] **ReLU** - Deep Sparse Rectified Neural Networks (2011), X. Glorot et al. [[PDF]](https://www.researchgate.net/publication/215616967_Deep_Sparse_Rectifier_Neural_Networks)
- [ ] **Residuals** - Deep Residual Learning for Image Recognition (2015), K. He et al. [[PDF]](https://arxiv.org/pdf/1512.03385)
- [ ] **Dropout** - Dropout: A Simple Way to Prevent Neural Networks from Overfitting (2014), N. Strivastava et al. [[PDF]](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)
- [ ] **BatchNorm** - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (2015), S. Ioffe and C. Szegedy [[PDF]](https://arxiv.org/pdf/1502.03167)
- [ ] **LayerNorm** - Layer Normalization (2016), J. Lei Ba et al. [[PDF]](https://arxiv.org/pdf/1607.06450)
- [ ] **GELU** - Gaussian Error Linear Units (GELUs) (2016), D. Hendrycks and K. Gimpel [[PDF]](https://arxiv.org/pdf/1606.08415)
- [ ] **Adam** - Adam: A Method for Stochastic Optimization (2014), D. P. Kingma and J. Ba [[PDF]](https://arxiv.org/pdf/1412.6980)
- [ ] **RNN** - A Learning Algorithm for Continually Running Fully Recurrent Neural Networks (1989), R. J. Williams [[PDF]](https://gwern.net/doc/ai/nn/rnn/1989-williams-2.pdf)
- [ ] **LSTM** - Long-Short Term Memory (1997), S. Hochreiter and J. Schmidhuber [[PDF]](https://www.bioinf.jku.at/publications/older/2604.pdf)
- [ ] **Learning to Forget** - Learning to Forget: Continual Prediction with LSTM (2000), F. A. Gers et al. [[PDF]](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e10f98b86797ebf6c8caea6f54cacbc5a50e8b34)

- [ ] **Word2Vec** - Efficient Estimation of Word Representations in Vector Space (2013), T. Mikolov et al. [[PDF]](https://arxiv.org/pdf/1301.3781)
- [ ] **Phrase2Vec** - Distributed Representations of Words and Phrases and their Compositionality (2013), T. Mikolov et al. [[PDF]](https://arxiv.org/pdf/1310.4546)
- [ ] **Encoder-Decoder** - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation (2014), K. Cho et al. [[PDF]](https://arxiv.org/pdf/1406.1078)
- [ ] **Seq2Seq** - Sequence to Sequence Learning with Neural Networks (2014), I. Sutskever et al. [[PDF]](https://arxiv.org/pdf/1409.3215)
- [ ] **Attention** - Neural Machine Translation by Jointly Learning to Align and Translate (2014), D. Bahdanau et al. [[PDF]](https://arxiv.org/pdf/1409.0473)
- [ ] **Mixture of Experts** - Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (2017), N. Shazeer et al. [[PDF]](https://arxiv.org/pdf/1701.06538)
- [ ] **Transformer** - Attention Is All You Need (2017), A. Vaswani et al. [[PDF]](https://arxiv.org/pdf/1706.03762)
- [ ] **BERT** - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018), J. Devlin et al. [[PDF]](https://arxiv.org/pdf/1810.04805)
- [ ] **RoBERTa** - RoBERTa: A Robustly Optimized BERT Pretraining Approach (2019), Y. Liu et al. [[PDF]](https://arxiv.org/pdf/1907.11692)
- [ ] **T5** - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (2019), C. Raffel et al. [[PDF]](https://arxiv.org/pdf/1910.10683)
- [ ] **BLEU** - BLEU: A Method for Automatic Evaluation of Machine Translation (2002) K. Papineni, S. Roukos, et al. [[PDF]](https://aclanthology.org/P02-1040.pdf)
- [ ] **ROUGE** - ROUGE: A Package for Automatic Evaluation of Summaries (2004) Chin-Yew Lin [[PDF]](https://aclanthology.org/W04-1013.pdf)
- [ ] **SuperGlue** - SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems (2019), A. Wang, Y. Pruksachatkun et al. [[PDF]](https://w4ngatang.github.io/static/papers/superglue.pdf)
- [ ] **BLOOM** - BLOOM: A 176B-Parameter Open-Access Multilingual Language Model (2023) [[PDF]](https://arxiv.org/pdf/2211.05100)
- [ ] **PEFT** - Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning (2023), V. Lialin, V. Deshpande, A. Rumshisky [[PDF]](https://arxiv.org/pdf/2303.15647)
- [ ] **PEFT Effectiveness** - On the Effectiveness of Parameter-Efficient Fine-Tuning (2022), Z. Fu, H. Yang et al. [[PDF]](https://arxiv.org/pdf/2211.15583)
- [ ] **LoRA** LoRA: Low-Rank Adaptation of Large Language Models (2021), E. J. Hu et al. [[PDF]](https://arxiv.org/pdf/2106.09685)
- [ ] **QLoRa** - QLoRA: Efficient Finetuning of Quantized LLMs (2023), T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer [[PDF]](https://arxiv.org/pdf/2305.14314)
- [ ] **Prompt Tuning** - The Power of Scale for Parameter-Efficient Prompt Tuning (2021), B. Lester, R. Al-Rfou, N. Constant [[PDF]](https://arxiv.org/pdf/2104.08691)
- [ ] **RLHF** - Fine-Tuning Language Models From Human Preferences (2019), D. Ziegler et al. [[PDF]](https://arxiv.org/pdf/1909.08593)
- [ ] **RLHF** - Training language models to follow instructions with human feedback (2022), L. Ouyang, J. Wu et al. [[PDF]](https://arxiv.org/pdf/2203.02155)
- [ ] **RLHF** - Learning to summarize from human feedback (2022), N. Stiennon, L. Ouyang, J. Wu, et al. [[PDF]](https://arxiv.org/pdf/2009.01325)

- [ ] **PPO** - Proximal Policy Optimization Algorithms (2017), J. Schulman et al. [[PDF]](https://arxiv.org/pdf/1707.06347)
- [ ] **InstructGPT** - Training language models to follow instructions with human feedback (2022), L. Ouyang et al. [[PDF]](https://arxiv.org/pdf/2203.02155)
- [ ] **Helpful & Harmless** - Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback (2022), Y. Bai et al. [[PDF]](https://arxiv.org/pdf/2204.05862)
- [ ] **Vision Transformer** - An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (2020), A. Dosovitskiy et al. [[PDF]](https://arxiv.org/pdf/2010.11929)
- [ ] **GAN** - Generative Adversarial Networks (2014), I. J. Goodfellow et al. [[PDF]](https://arxiv.org/pdf/1406.2661)
- [ ] **Anime Gan** - Towards the Automatic Anime Characters Creation with Generative Adversarial Networks (2017) Y. Jin, J. Zhanf et al. [[PDF]](https://arxiv.org/pdf/1708.05509v1)
- [ ] **VAE** - Auto-Encoding Variational Bayes (2013), D. Kingma and M. Welling [[PDF]](https://arxiv.org/pdf/1312.6114)
- [ ] **VQ VAE** - Neural Discrete Representation Learning (2017), A. Oord et al. [[PDF]](https://arxiv.org/pdf/1711.00937)
- [ ] **VQ VAE 2** - Generating Diverse High-Fidelity Images with VQ-VAE-2 (2019), A. Razavi et al. [[PDF]](https://arxiv.org/pdf/1906.00446)
- [ ] **Diffusion** - Deep Unsupervised Learning using Nonequilibrium Thermodynamics (2015), J. Sohl-Dickstein et al. [[PDF]](https://arxiv.org/pdf/1503.03585)
- [ ] **Denoising Diffusion** - Denoising Diffusion Probabilistic Models (2020), J. Ho. et al. [[PDF]](https://arxiv.org/pdf/2006.11239)
- [ ] **Denoising Diffusion 2** - Improved Denoising Diffusion Probabilistic Models (2021), A. Nichol and P. Dhariwal [[PDF]](https://arxiv.org/pdf/2102.09672)
- [ ] **Diffusion Beats GANs** - Diffusion Models Beat GANs on Image Synthesis, P. Dhariwal and A. Nichol [[PDF]](https://arxiv.org/pdf/2105.05233)
- [ ] **CLIP** - Learning Transferable Visual Models From Natural Language Supervision (2021), A. Radford et al. [[PDF]](https://arxiv.org/pdf/2103.00020)
- [ ] **DALL E** - Zero-Shot Text-to-Image Generation (2021), A. Ramesh et al. [[PDF]](https://arxiv.org/pdf/2102.12092)
- [ ] **DALL E 2** - Hierarchical Text-Conditional Image Generation with CLIP Latents (2022), A. Ramesh et al. [[PDF]](https://arxiv.org/pdf/2204.06125)
- [ ] **Deep Learning** - Deep Learning (2015), Y. LeCun, Y. Bengio, and G. Hinton [[PDF]](https://www.nature.com/articles/nature14539.pdf)
- [ ] **GAN** - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (2016), A. Radford et al. [[PDF]](https://arxiv.org/pdf/1511.06434)
- [ ] **DCGAN** - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (2016), A. Radford et al. [[PDF]](https://arxiv.org/pdf/1511.06434)
- [ ] **BigGAN** - Large Scale GAN Training for High Fidelity Natural Image Synthesis (2018), A. Brock et al. [[PDF]](https://arxiv.org/pdf/1809.11096)
- [ ] **WaveNet** - WaveNet: A Generative Model for Raw Audio (2016), A. van den Oord et al. [[PDF]](https://arxiv.org/pdf/1609.03499)
- [ ] **BERTology** - A Survey of BERT Use Cases (2020), R. Rogers et al. [[PDF]](https://arxiv.org/pdf/2002.12327)
- [ ] **GPT** - Improving Language Understanding by Generative Pre-Training (2018), A. Radford et al. [[PDF]](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [ ] **GPT-2** - Language Models are Unsupervised Multitask Learners (2018), A. Radford et al. [[PDF]](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [ ] **GPT-3** - Language Models are Few-Shot Learners (2020) T. B. Brown et al. [[PDF]](https://arxiv.org/pdf/2005.14165)
- [ ] **GPT-4** - GPT-4 Technical Report (2023), OpenAI [[PDF]](https://arxiv.org/pdf/2303.08774)
- [ ] **Deep Reinforcement Learning** - Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm (2017), D. Silver et al. [[PDF]](https://arxiv.org/pdf/1712.01815)
- [ ] **Deep Q-Learning** - Playing Atari with Deep Reinforcement Learning (2013), V. Mnih et al. [[PDF]](https://arxiv.org/pdf/1312.5602)
- [ ] **AlphaGo** - Mastering the Game of Go with Deep Neural Networks and Tree Search (2016), D. Silver et al. [[PDF]](https://www.nature.com/articles/nature16961)
- [ ] **AlphaFold** - Highly accurate protein structure prediction with AlphaFold (2021), J. Jumper et al. [[PDF]](https://www.nature.com/articles/s41586-021-03819-2)
- [ ] **T5** - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (2019), C. Raffel et al. [[PDF]](https://arxiv.org/pdf/1910.10683)
- [ ] **ELECTRA** - ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators (2020), K. Clark et al. [[PDF]](https://arxiv.org/pdf/2003.10555)
- [ ] **SimCLR** - A Simple Framework for Contrastive Learning of Visual Representations (2020), T. Chen et al. [[PDF]](https://arxiv.org/pdf/2002.05709)



